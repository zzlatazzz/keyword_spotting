{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lhrn5O-qUYZ"
   },
   "source": [
    "# Import and misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "meO-Mp9jiAFC"
   },
   "outputs": [],
   "source": [
    "# Instal latest torch and torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bbUpoArCqUYa"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Union, List, Callable, Optional\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import pathlib\n",
    "import dataclasses\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torchaudio\n",
    "from IPython import display as display_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "812GwLfqqUYf"
   },
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8PdhApeEh9pH"
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class TaskConfig:\n",
    "    keyword: str = 'sheila'  # We will use 1 key word -- 'sheila'\n",
    "    batch_size: int = 128\n",
    "    learning_rate: float = 3e-4\n",
    "    weight_decay: float = 1e-5\n",
    "    num_epochs: int = 20\n",
    "    n_mels: int = 40\n",
    "    cnn_out_channels: int = 8\n",
    "    kernel_size: Tuple[int, int] = (5, 20)\n",
    "    stride: Tuple[int, int] = (2, 8)\n",
    "    hidden_size: int = 64\n",
    "    gru_num_layers: int = 2\n",
    "    bidirectional: bool = False\n",
    "    num_classes: int = 2\n",
    "    sample_rate: int = 16000\n",
    "    device: torch.device = torch.device(\n",
    "        'cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KA1gPmE1h9pI"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y2N8zcx9MF1X",
    "outputId": "45c84b64-e66f-414e-b95f-09bad1474a2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-05 12:22:25--  http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\n",
      "Resolving download.tensorflow.org (download.tensorflow.org)... 172.217.164.176, 2607:f8b0:4004:815::2010\n",
      "Connecting to download.tensorflow.org (download.tensorflow.org)|172.217.164.176|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1489096277 (1.4G) [application/gzip]\n",
      "Saving to: ‘speech_commands_v0.01.tar.gz’\n",
      "\n",
      "speech_commands_v0. 100%[===================>]   1.39G   190MB/s    in 6.4s    \n",
      "\n",
      "2022-11-05 12:22:32 (220 MB/s) - ‘speech_commands_v0.01.tar.gz’ saved [1489096277/1489096277]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz -O speech_commands_v0.01.tar.gz\n",
    "!mkdir speech_commands && tar -C speech_commands -xvzf speech_commands_v0.01.tar.gz 1> log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "12wBTK0mNUsG"
   },
   "outputs": [],
   "source": [
    "class SpeechCommandDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform: Optional[Callable] = None,\n",
    "        path2dir: str = None,\n",
    "        keywords: Union[str, List[str]] = None,\n",
    "        csv: Optional[pd.DataFrame] = None\n",
    "    ):        \n",
    "        self.transform = transform\n",
    "\n",
    "        if csv is None:\n",
    "            path2dir = pathlib.Path(path2dir)\n",
    "            keywords = keywords if isinstance(keywords, list) else [keywords]\n",
    "            \n",
    "            all_keywords = [\n",
    "                p.stem for p in path2dir.glob('*')\n",
    "                if p.is_dir() and not p.stem.startswith('_')\n",
    "            ]\n",
    "\n",
    "            triplets = []\n",
    "            for keyword in all_keywords:\n",
    "                paths = (path2dir / keyword).rglob('*.wav')\n",
    "                if keyword in keywords:\n",
    "                    for path2wav in paths:\n",
    "                        triplets.append((path2wav.as_posix(), keyword, 1))\n",
    "                else:\n",
    "                    for path2wav in paths:\n",
    "                        triplets.append((path2wav.as_posix(), keyword, 0))\n",
    "            \n",
    "            self.csv = pd.DataFrame(\n",
    "                triplets,\n",
    "                columns=['path', 'keyword', 'label']\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.csv = csv\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        instance = self.csv.iloc[index]\n",
    "\n",
    "        path2wav = instance['path']\n",
    "        wav, sr = torchaudio.load(path2wav)\n",
    "        wav = wav.sum(dim=0)\n",
    "        \n",
    "        if self.transform:\n",
    "            wav = self.transform(wav)\n",
    "\n",
    "        return {\n",
    "            'wav': wav,\n",
    "            'keywors': instance['keyword'],\n",
    "            'label': instance['label']\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-1rVkT81Pk90"
   },
   "outputs": [],
   "source": [
    "dataset = SpeechCommandDataset(\n",
    "    path2dir='speech_commands', keywords=TaskConfig.keyword\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "DFwhAXdfQLIA",
    "outputId": "c8faeca6-79a6-43ca-a6bb-e0397e1e0103"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-355de4b0-5222-4560-8476-2d2942307857\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>keyword</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18462</th>\n",
       "      <td>speech_commands/seven/01bb6a2a_nohash_3.wav</td>\n",
       "      <td>seven</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21742</th>\n",
       "      <td>speech_commands/one/70a00e98_nohash_3.wav</td>\n",
       "      <td>one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17361</th>\n",
       "      <td>speech_commands/four/324210dd_nohash_3.wav</td>\n",
       "      <td>four</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63688</th>\n",
       "      <td>speech_commands/zero/035de8fe_nohash_0.wav</td>\n",
       "      <td>zero</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29549</th>\n",
       "      <td>speech_commands/nine/742d6431_nohash_2.wav</td>\n",
       "      <td>nine</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-355de4b0-5222-4560-8476-2d2942307857')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-355de4b0-5222-4560-8476-2d2942307857 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-355de4b0-5222-4560-8476-2d2942307857');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                              path keyword  label\n",
       "18462  speech_commands/seven/01bb6a2a_nohash_3.wav   seven      0\n",
       "21742    speech_commands/one/70a00e98_nohash_3.wav     one      0\n",
       "17361   speech_commands/four/324210dd_nohash_3.wav    four      0\n",
       "63688   speech_commands/zero/035de8fe_nohash_0.wav    zero      0\n",
       "29549   speech_commands/nine/742d6431_nohash_2.wav    nine      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.csv.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUxfDJw1qUYi"
   },
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dkmkxPWQqUYe"
   },
   "outputs": [],
   "source": [
    "class AugsCreation:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.background_noises = [\n",
    "            'speech_commands/_background_noise_/white_noise.wav',\n",
    "            'speech_commands/_background_noise_/dude_miaowing.wav',\n",
    "            'speech_commands/_background_noise_/doing_the_dishes.wav',\n",
    "            'speech_commands/_background_noise_/exercise_bike.wav',\n",
    "            'speech_commands/_background_noise_/pink_noise.wav',\n",
    "            'speech_commands/_background_noise_/running_tap.wav'\n",
    "        ]\n",
    "\n",
    "        self.noises = [\n",
    "            torchaudio.load(p)[0].squeeze()\n",
    "            for p in self.background_noises\n",
    "        ]\n",
    "\n",
    "    def add_rand_noise(self, audio):\n",
    "\n",
    "        # randomly choose noise\n",
    "        noise_num = torch.randint(low=0, high=len(\n",
    "            self.background_noises), size=(1,)).item()\n",
    "        noise = self.noises[noise_num]\n",
    "\n",
    "        noise_level = torch.Tensor([1])  # [0, 40]\n",
    "\n",
    "        noise_energy = torch.norm(noise)\n",
    "        audio_energy = torch.norm(audio)\n",
    "        alpha = (audio_energy / noise_energy) * \\\n",
    "            torch.pow(10, -noise_level / 20)\n",
    "\n",
    "        start = torch.randint(\n",
    "            low=0,\n",
    "            high=max(int(noise.size(0) - audio.size(0) - 1), 1),\n",
    "            size=(1,)\n",
    "        ).item()\n",
    "        noise_sample = noise[start: start + audio.size(0)]\n",
    "\n",
    "        audio_new = audio + alpha * noise_sample\n",
    "        audio_new.clamp_(-1, 1)\n",
    "        return audio_new\n",
    "\n",
    "    def __call__(self, wav):\n",
    "        aug_num = torch.randint(low=0, high=4, size=(1,)).item()   # choose 1 random aug from augs\n",
    "        augs = [\n",
    "            lambda x: x,\n",
    "            lambda x: (x + distributions.Normal(0, 0.01).sample(x.size())).clamp_(-1, 1),\n",
    "            lambda x: torchaudio.transforms.Vol(.25)(x),\n",
    "            lambda x: self.add_rand_noise(x)\n",
    "        ]\n",
    "\n",
    "        return augs[aug_num](wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ClWThxyYh9pM"
   },
   "outputs": [],
   "source": [
    "indexes = torch.randperm(len(dataset))\n",
    "train_indexes = indexes[:int(len(dataset) * 0.8)]\n",
    "val_indexes = indexes[int(len(dataset) * 0.8):]\n",
    "\n",
    "train_df = dataset.csv.iloc[train_indexes].reset_index(drop=True)\n",
    "val_df = dataset.csv.iloc[val_indexes].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PDPLht5fqUYe"
   },
   "outputs": [],
   "source": [
    "# Sample is a dict of utt, word and label\n",
    "train_set = SpeechCommandDataset(csv=train_df, transform=AugsCreation())\n",
    "val_set = SpeechCommandDataset(csv=val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mmrJd8WIhkLP",
    "outputId": "96208e87-744f-4889-81af-8a684cf56fc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wav': tensor([ 0.0036,  0.0070,  0.0147,  ..., -0.0006, -0.0228, -0.0144]),\n",
       " 'keywors': 'left',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vbPDqd6qUYj"
   },
   "source": [
    "### Sampler for oversampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rfnjRKo2qUYj"
   },
   "outputs": [],
   "source": [
    "# We should provide to WeightedRandomSampler _weight for every sample_; by default it is 1/len(target)\n",
    "\n",
    "def get_sampler(target):\n",
    "    class_sample_count = np.array(\n",
    "        [len(np.where(target == t)[0]) for t in np.unique(target)])   # for every class count it's number of occ.\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in target])\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    samples_weigth = samples_weight.float()\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UM8gLmHeqUYj"
   },
   "outputs": [],
   "source": [
    "train_sampler = get_sampler(train_set.csv['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lyBqbxp0h9pO"
   },
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        wavs = []\n",
    "        labels = []    \n",
    "\n",
    "        for el in data:\n",
    "            wavs.append(el['wav'])\n",
    "            labels.append(el['label'])\n",
    "\n",
    "        # torch.nn.utils.rnn.pad_sequence takes list(Tensors) and returns padded (with 0.0) Tensor\n",
    "        wavs = pad_sequence(wavs, batch_first=True)    \n",
    "        labels = torch.Tensor(labels).long()\n",
    "        return wavs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8G9xPRVqUYk"
   },
   "source": [
    "###  Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6wGBMcQiqUYk"
   },
   "outputs": [],
   "source": [
    "# Here we are obliged to use shuffle=False because of our sampler with randomness inside.\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=TaskConfig.batch_size,\n",
    "                          shuffle=False, collate_fn=Collator(),\n",
    "                          sampler=train_sampler,\n",
    "                          num_workers=2, pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(val_set, batch_size=TaskConfig.batch_size,\n",
    "                        shuffle=False, collate_fn=Collator(),\n",
    "                        num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTlsn6cpqUYk"
   },
   "source": [
    "### Creating MelSpecs on GPU for speeeed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "pRXMt6it56fW"
   },
   "outputs": [],
   "source": [
    "class LogMelspec:\n",
    "\n",
    "    def __init__(self, is_train, config):\n",
    "        # with augmentations\n",
    "        if is_train:\n",
    "            self.melspec = nn.Sequential(\n",
    "                torchaudio.transforms.MelSpectrogram(\n",
    "                    sample_rate=config.sample_rate,\n",
    "                    n_fft=400,\n",
    "                    win_length=400,\n",
    "                    hop_length=160,\n",
    "                    n_mels=config.n_mels\n",
    "                ),\n",
    "                torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "                torchaudio.transforms.TimeMasking(time_mask_param=35),\n",
    "            ).to(config.device)\n",
    "\n",
    "        # no augmentations\n",
    "        else:\n",
    "            self.melspec = torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=config.sample_rate,\n",
    "                n_fft=400,\n",
    "                win_length=400,\n",
    "                hop_length=160,\n",
    "                n_mels=config.n_mels\n",
    "            ).to(config.device)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # already on device\n",
    "        return torch.log(self.melspec(batch).clamp_(min=1e-9, max=1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Pqkz4_gn8BiF"
   },
   "outputs": [],
   "source": [
    "melspec_train = LogMelspec(is_train=True, config=TaskConfig)\n",
    "melspec_val = LogMelspec(is_train=False, config=TaskConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoAxmihY8yxr"
   },
   "source": [
    "### Quality measurment functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "euwD1UyuqUYk"
   },
   "outputs": [],
   "source": [
    "# FA - true: 0, model: 1\n",
    "# FR - true: 1, model: 0\n",
    "\n",
    "def count_FA_FR(preds, labels):\n",
    "    FA = torch.sum(preds[labels == 0])\n",
    "    FR = torch.sum(labels[preds == 0])\n",
    "    \n",
    "    # torch.numel - returns total number of elements in tensor\n",
    "    return FA.item() / torch.numel(preds), FR.item() / torch.numel(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "YHBUrkT1qUYk"
   },
   "outputs": [],
   "source": [
    "def get_au_fa_fr(probs, labels):\n",
    "    sorted_probs, _ = torch.sort(probs)\n",
    "    sorted_probs = torch.cat((torch.Tensor([0]), sorted_probs, torch.Tensor([1])))\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "        \n",
    "    FAs, FRs = [], []\n",
    "    for prob in sorted_probs:\n",
    "        preds = (probs >= prob) * 1\n",
    "        FA, FR = count_FA_FR(preds, labels)        \n",
    "        FAs.append(FA)\n",
    "        FRs.append(FR)\n",
    "    # plt.plot(FAs, FRs)\n",
    "    # plt.show()\n",
    "\n",
    "    # ~ area under curve using trapezoidal rule\n",
    "    return -np.trapz(FRs, x=FAs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcEP5cEZqUYl"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2cP_pFIsy5p2"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.energy = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        energy = self.energy(input)\n",
    "        alpha = torch.softmax(energy, dim=-2)\n",
    "        return (input * alpha).sum(dim=-2)\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oifgnRC1v4Lh",
    "outputId": "ca8804df-4845-4b92-cf13-9d53e05b9559"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRNN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(5, 20), stride=(2, 8))\n",
       "    (1): Flatten(start_dim=1, end_dim=2)\n",
       "  )\n",
       "  (gru): GRU(144, 64, num_layers=2, batch_first=True, dropout=0.1)\n",
       "  (attention): Attention(\n",
       "    (energy): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tconfig = TaskConfig()\n",
    "teacher = torch.load('teacher_model.pth', map_location=tconfig.device)\n",
    "teacher.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGK7HJJXwRDr"
   },
   "source": [
    "## StudentConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "3FCYWTP5wTw_"
   },
   "outputs": [],
   "source": [
    "# Уменьшаем параметры у студента\n",
    "# После решетки указаны старые параметры\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class StudentConfig:\n",
    "    keyword: str = 'sheila'  # We will use 1 key word -- 'sheila'\n",
    "    batch_size: int = 128\n",
    "    learning_rate: float = 1e-3 # 3e-4\n",
    "    weight_decay: float = 1e-5 # 1e-5\n",
    "    num_epochs: int = 40 # 20\n",
    "    n_mels: int = 40\n",
    "    cnn_out_channels: int = 2 # 8\n",
    "    kernel_size: Tuple[int, int] = (5, 20) # (5, 20)\n",
    "    stride: Tuple[int, int] = (2, 8) # (2, 8)\n",
    "    hidden_size: int = 22 # 64\n",
    "    gru_num_layers: int = 1 # 2\n",
    "    bidirectional: bool = False\n",
    "    num_classes: int = 2\n",
    "    sample_rate: int = 16000\n",
    "    device: torch.device = torch.device(\n",
    "        'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    T: float = 10\n",
    "    a: float = 0.6\n",
    "\n",
    "\n",
    "sconfig = StudentConfig()\n",
    "student = CRNN(sconfig).to(sconfig.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "Kf3_0Q1vw5bh"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "history = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "gefe7X78wgN1"
   },
   "outputs": [],
   "source": [
    "sopt = torch.optim.Adam(\n",
    "    student.parameters(),\n",
    "    lr=sconfig.learning_rate,\n",
    "    weight_decay=sconfig.weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSNW-nZCJ4Q0"
   },
   "source": [
    "# Student training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "vt2kjqC-IobK"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "Uz2hRvhiuzoZ"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Timer:\n",
    "\n",
    "    def __init__(self, name: str, verbose=False):\n",
    "        self.name = name\n",
    "        self.verbose = verbose\n",
    "        self.total_time = 0\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.t = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.t = time.time() - self.t\n",
    "        self.total_time += self.t\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"{self.name.capitalize()} | Elapsed time : {self.t:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "DmmSFvWaqUYn"
   },
   "outputs": [],
   "source": [
    "def dkd_train_epoch(teacher, student, opt, loader, log_melspec, device, T, a):\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    for i, (batch, labels) in tqdm(enumerate(loader), total=len(loader)):\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        batch = log_melspec(batch)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits_student = student(batch)\n",
    "            with torch.no_grad():\n",
    "                logits_teacher = teacher(batch)\n",
    "\n",
    "        # distillation loss + student loss\n",
    "        with torch.cuda.amp.autocast():\n",
    "            probs_student = F.log_softmax(logits_student / T, dim=-1)\n",
    "            probs_teacher = F.log_softmax(logits_teacher / T, dim=-1)\n",
    "  \n",
    "            probs = F.softmax(logits_student.to(float), dim=-1)\n",
    "        \n",
    "            loss = nn.KLDivLoss()(probs_student, probs_teacher) * (T ** 2) * a + F.cross_entropy(logits_student, labels) * (1 - a)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(opt)\n",
    "        torch.nn.utils.clip_grad_norm_(student.parameters(), 5)\n",
    "\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        # logging\n",
    "        argmax_probs = torch.argmax(probs, dim=-1)\n",
    "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
    "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "UIeRbn4tqUYo"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def dkd_validation(teacher, student, loader, log_melspec, device, T, a):\n",
    "    student.eval()\n",
    "    teacher.eval()\n",
    "    #scaler = torch.cuda.amp.GradScaler()\n",
    "    val_losses, accs, FAs, FRs = [], [], [], []\n",
    "    all_probs, all_labels = [], []\n",
    "    for i, (batch, labels) in tqdm(enumerate(loader)):\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        batch = log_melspec(batch)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits_student = student(batch.half())\n",
    "            logits_teacher = teacher(batch)\n",
    "\n",
    "            # distillation loss + student loss\n",
    "            probs_student = F.log_softmax(logits_student / T, dim=-1)\n",
    "            probs_teacher = F.log_softmax(logits_teacher / T, dim=-1)\n",
    "            probs = F.softmax(logits_student.to(float), dim=-1)\n",
    "            \n",
    "            loss = nn.KLDivLoss()(probs_student, probs_teacher) * (T ** 2) * a + F.cross_entropy(logits_student, labels) * (1 - a)\n",
    "\n",
    "        # logging\n",
    "        argmax_probs = torch.argmax(probs, dim=-1)\n",
    "        all_probs.append(probs[:, 1].cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "        val_losses.append(loss.item())\n",
    "        accs.append(\n",
    "            torch.sum(argmax_probs == labels).item() /  # ???\n",
    "            torch.numel(argmax_probs)\n",
    "        )\n",
    "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
    "        FAs.append(FA)\n",
    "        FRs.append(FR)\n",
    "\n",
    "    # area under FA/FR curve for whole loader\n",
    "    au_fa_fr = get_au_fa_fr(torch.cat(all_probs, dim=0).cpu(), all_labels)\n",
    "    return au_fa_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "MsY9uiXqxZ1X",
    "outputId": "dd034e21-375b-40c9-dee5-744406d00b12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student | Elapsed time : 877.95\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEGCAYAAACzYDhlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxV1b3//9fnnAyEkIQ5CTMKGAYniKDV1tQRbCu2agV7r9aidBBrve1ttb0/2uu33n7t/bb2OrXFaqvWitYO4i2KY9SqCKKAjBpBmecxIIEkn98fZ4PHmJOEJPuck+T9fDzOg33WXmvtzz6EfNh7r7OWuTsiIiJhiqQ6ABERaf+UbEREJHRKNiIiEjolGxERCZ2SjYiIhC4j1QGko549e/qgQYOa1Xbfvn3k5ua2bkCtKN3jg/SPUfG1jOJrmXSOb8GCBdvcvVe9O91drzqvMWPGeHO98MILzW6bDOken3v6x6j4WkbxtUw6xwe84Ql+r+o2moiIhE7JRkREQqdkIyIioVOyERGR0CnZiIhI6JRsREQkdEo2IiISOiWbVrRy014eXXmQyqrqVIciIpJWlGxa0dod+5m9+hArN+1JdSgiImlFyaYVDe+TD8DyjXtTHImISHpRsmlFfQo6kZMByzfqykZEJF6oycbMxpvZSjOrMLMb69mfbWaPBPtfN7NBcftuCspXmtn5R9Hn7WZW2ZRjtDYzo39ehBWbdGUjIhIvtGRjZlHgLmACMAKYbGYj6lSbAux09yHAbcCtQdsRwCRgJDAeuNvMoo31aWalQLemHCMs/fMirNy0l9paD/MwIiJtSphXNmOBCndf5e4HgZnAxDp1JgL3B9uPAWebmQXlM929yt1XAxVBfwn7DBLRfwPfb+IxQtE/L0JlVTXrdn4Y1iFERNqcMNez6QusjXu/DhiXqI67V5vZbqBHUD63Ttu+wXaiPqcBs9x9Y51ckugY2+IrmdlUYCpAYWEh5eXlTT3Pj+mZUQUYf372VcYUpt9yQZWVlc0+t2RJ9xgVX8sovpZJ9/gSSb/fhs1gZn2AS4Gy5vbh7jOAGQClpaVeVta8rqqefQGz/UR7DKCsbFhzwwlNeXk5zT23ZEn3GBVfyyi+lkn3+BIJ8zbaeqB/3Pt+QVm9dcwsAygAtjfQNlH5ycAQoMLM3gc6m1lFI8cIRXaGMahHLis0/FlE5Igwk818YKiZDTazLGIP/GfVqTMLuDLYvgR4PljtbRYwKRhJNhgYCsxL1Ke7/8Pdi9x9kLsPAvYHAwIaOkZoSoryWKEvdoqIHBHabbTg+cg0YA4QBe5z96VmdjOxpUNnAfcCDwZXITuIJQ+Ceo8Cy4Bq4Fp3rwGor89GQqn3GGEaXpzPU0s3sa+qmtzsdnGnUkSkRUL9Tejus4HZdcqmx20fIPaspb62twC3NKXPeup0acoxwlJSlIc7rNy8l9ED6o7EFhHpeDSDQAiGF8emrdFzGxGRGCWbEPTrlkOX7AxNWyMiElCyCYGZaZCAiEgcJZuQDC/OZ8XGvYQ88E1EpE1QsglJSXEeezVtjYgIoGQTmpKiYJCAZoAWEVGyCUtJUR6gtW1EREDJJjS52RkM7NFZgwRERFCyCdXwonx910ZEBCWbUJUU57F6+z72H6xOdSgiIimlZBOikqJ83OGdzZWNVxYRaceUbEI04si0NXpuIyIdm5JNiPp1yyE3K6oRaSLS4SnZhCgSMY4rymO5vmsjIh2ckk3IYtPW7NG0NSLSoSnZhKykOJ89B6rZsPtAqkMREUkZJZuQDQ9mEtAgARHpyJRsQnacpq0REQk32ZjZeDNbaWYVZnZjPfuzzeyRYP/rZjYobt9NQflKMzu/sT7N7F4zW2Rmi83sMTPrEpR/1cy2mtnC4HV1mOdcV16nTPp3z9EgARHp0EJLNmYWBe4CJgAjgMlmNqJOtSnATncfAtwG3Bq0HQFMAkYC44G7zSzaSJ83uPuJ7n4CsAaYFnecR9z9pOD1uzDOtyGxaWt0ZSMiHVeYVzZjgQp3X+XuB4GZwMQ6dSYC9wfbjwFnm5kF5TPdvcrdVwMVQX8J+3T3PQBB+xwgbYZ/lRTns3rbPg4cqkl1KCIiKRFmsukLrI17vy4oq7eOu1cDu4EeDbRtsE8z+z2wCSgB7oird3Hc7bX+LTinZhlelEetwzubdStNRDqmjFQH0Jrc/argVtsdwGXA74EngIfdvcrMvk7sSuqsum3NbCowFaCwsJDy8vJmxVBZWfmJtnv21QLw9/L57OiX2ax+W0t98aWbdI9R8bWM4muZdI8vkTCTzXog/iqiX1BWX511ZpYBFADbG2nbYJ/uXmNmM4HvA7939+1xu38H/Ly+YN19BjADoLS01MvKyho5vfqVl5dTt21trfOTuXOoze9DWdnIZvXbWuqLL92ke4yKr2UUX8uke3yJhHkbbT4w1MwGm1kWsQf+s+rUmQVcGWxfAjzvsa/azwImBaPVBgNDgXmJ+rSYIXDkmc2FwIrgfXHc8S4Elodwrg06PG2NFlITkY4qtCsbd682s2nAHCAK3OfuS83sZuANd58F3As8aGYVwA5iyYOg3qPAMqAauNbdawAS9BkB7jezfMCARcA3g1C+bWYXBv3sAL4a1jk3ZHhxPk8u2Yi7E8uHIiIdR6jPbNx9NjC7Ttn0uO0DwKUJ2t4C3NLEPmuB0xP0cxNw09HG3tqGF+fx8Lw1bNpzgOKCnFSHIyKSVJpBIElKig6vbaMRaSLS8SjZJElJcWzammX6cqeIdEBKNkmS3ymTvl1zWKFpa0SkA1KySaLDa9uIiHQ0SjZJNLw4j1WatkZEOiAlmyQqKcqnptap2FKZ6lBERJJKySaJhhdrbRsR6ZiUbJJoYI9cOmVGWK7hzyLSwSjZJFE0YhxXqGlrRKTjUbJJsuHF+SzfuIfYFHAiIh2Dkk2SlRTlsXP/IbbsrUp1KCIiSaNkk2TDi2PT1miQgIh0JEo2SXZ4jjQNEhCRjkTJJskKOmfSp6CTBgmISIeiZJMCsWlrdGUjIh2Hkk0KlBTn8d7WSqqqNW2NiHQMSjYpUFKUT7WmrRGRDkTJJgUOj0jTrTQR6SiUbFJgUI/OZGdENPxZRDqMUJONmY03s5VmVmFmN9azP9vMHgn2v25mg+L23RSUrzSz8xvr08zuNbNFZrbYzB4zsy6NHSNVMqIRhhXmaSE1EekwQks2ZhYF7gImACOAyWY2ok61KcBOdx8C3AbcGrQdAUwCRgLjgbvNLNpInze4+4nufgKwBpjW0DFSbXix5kgTkY4jzCubsUCFu69y94PATGBinToTgfuD7ceAs83MgvKZ7l7l7quBiqC/hH26+x6AoH0O4I0cI6VKivLZVnmQLXsPpDoUEZHQZYTYd19gbdz7dcC4RHXcvdrMdgM9gvK5ddr2DbYT9mlmvwcuAJYB323kGNviAzGzqcBUgMLCQsrLy5t+pnEqKyub1LZqe2zY86Nz/smonmH+NXxcU+NLpXSPUfG1jOJrmXSPL5Hk/ZZLAne/KrjVdgdwGfD7o2g7A5gBUFpa6mVlZc2Koby8nKa0PXHfQW6d/wyZvQZTduaxzTpWczQ1vlRK9xgVX8sovpZJ9/gSCfM22nqgf9z7fkFZvXXMLAMoALY30LbRPt29htjttYsbOUZKdcvNoii/kwYJiEiHEGaymQ8MNbPBZpZF7IH/rDp1ZgFXBtuXAM97bKGXWcCkYCTZYGAoMC9RnxYzBI48s7kQWNHIMVJueHGehj+LSIcQ2m204PnINGAOEAXuc/elZnYz8Ia7zwLuBR40swpgB7HkQVDvUWLPXqqBa4MrFhL0GQHuN7N8wIBFwDeDUOo9RjooKc7nnxXbOFhdS1aGvvIkIu1XqM9s3H02MLtO2fS47QPApQna3gLc0sQ+a4HTE/ST8BipVlKUx6Ea572tlUdmFRARaY/03+kUGnF42hp930ZE2jklmxQa3DOXrGhEC6mJSLunZJNCGdEIQwu7aJCAiLR7SjYpNrw4X8OfRaTdU7JJsZKiPLburWJbZVWqQxERCY2STYqN0No2ItIBKNmk2HFFeQB6biMi7ZqSTYr16JJN77xslmv4s4i0Y0o2aWB4cb5uo4lIu6ZkkwZKivOo2FLJoZraVIciIhIKJZs0MLwon4M1tazaui/VoYiIhELJJg0M17Q1ItLOKdmkgWN6xaatWaYRaSLSTinZpIHMaIQhvbtokICItFtKNmmipDhPt9FEpN1SskkTw4vy2bynih37DqY6FBGRVqdkkyaODBLQcxsRaYeUbNJESXFs2hoNEhCR9kjJJk307JJNzy7ZWm5ARNqlJiUbM/uimRXEve9qZhc1od14M1tpZhVmdmM9+7PN7JFg/+tmNihu301B+UozO7+xPs3soaB8iZndZ2aZQXmZme02s4XBa3pTzjkVhmuQgIi0U029svmxu+8+/MbddwE/bqiBmUWBu4AJwAhgspmNqFNtCrDT3YcAtwG3Bm1HAJOAkcB44G4zizbS50NACXA8kANcHXecl939pOB1cxPPOemGF+fzzuZKqjVtjYi0M01NNvXVy2ikzVigwt1XuftBYCYwsU6dicD9wfZjwNlmZkH5THevcvfVQEXQX8I+3X22B4B5QL8mnlvaGFGcz8HqWp5YvCHVoYiItKrGEsZhb5jZL4ldVQBcCyxopE1fYG3c+3XAuER13L3azHYDPYLyuXXa9g22G+wzuH32r8D1ccWnmdkiYAPwPXdfWjdYM5sKTAUoLCykvLy8kdOrX2VlZbPb5tQ4w7pF+O6ji1i5YgWnFjf1r6fpWhJfsqR7jIqvZRRfy6R7fIk09bfZdcD/BzwSvH+GWMJJR3cDL7n7y8H7N4GB7l5pZhcAfweG1m3k7jOAGQClpaVeVlbWrIOXl5fT3LYAZ5xRzVV/mM+MxTsoKRnORSf3bbzRUWhpfMmQ7jEqvpZRfC2T7vEl0qRk4+77gE884G/EeqB/3Pt+QVl9ddaZWQZQAGxvpG3CPs3sx0Av4Otxse+J255tZnebWU9333aU55MUudkZ/OGqU5jyhze44dGF1NQ6F49pc3cEj8qHB2tYtG4Xb67ZyQfb9vPpfE91SCLSyhpMNmb2K3f/jpk9AXziN4C7X9hA8/nAUDMbTCwhTAIur1NnFnAl8BpwCfC8u7uZzQL+FNy660PsSmQeYIn6NLOrgfOBs939yBN2MysCNgf9jiX2/Gl7Q+edap2zMrjvq6dwzQNv8L3HFlHjzpdL+zfesI3YsOtDFnywkwUf7OTNNTtZtmEP1bUf/XhFSrL4fArjE5HW19iVzYPBn//vaDsOnsFMA+YAUeA+d19qZjcDb7j7LOBe4EEzqwB2EEseBPUeBZYB1cC17l4DUF+fwSF/A3wAvBYbY8Bfg5FnlwDfNLNq4ENgUjCIIK3lZEX53ZWlXPPAG3z/scXU1DqTxw5IdVhH7VBNLcs27IkllzU7efODnWzcfQCAnMwoJ/Yv4OtnHsOYgd04uX83Jt8zlwWbta6PSHvTYLJx9wXBcOOp7v6Vo+3c3WcDs+uUTY/bPgBcmqDtLcAtTekzKK/3XNz9TuDOowo8TXTKjHLPFaV8448LuOmvb1NT6/zLqQNTHVaDtldW8daaXSxYE7tyWbxuFwcOxS40+3bNoXRQd8YM6MqYgd0pKc4jM/rxgY4TRhXzq2ffYcveA/TO65SKUxCREDT6zMbda8xsoJllBcONJYk6ZUb57b+O4Zt/fJP/+PsSat254rRBKYunttbZsreKD7bv44Md+1mzfX/wZ+z9rv2HAMiIGCP7FnD52IGMGdiN0QO7UlyQ02j/40cVcduz7/DMss18ZVx6J1YRabqmjkZbBbwSPEs5co/D3X8ZSlTyMdkZUX79L6OZ9qe3mP74UmpqnatOHxza8Q5W17Ju5/6Pksn2/azZsS/4cz9V1R996TQaMfp07cTA7rlccHwxg3vkcmL/rpzQr4BOmdGjPvawwi4UdjaeWrJJyUakHWlqsnkveEWAvKAs7Z97tCfZGVHuunw01z38Jv/5xDJqap2rP31Mq/W/bud+Hp63hv9dvJG1O/YT97yenMwoA7p3ZlDPXM4c1ouBPTozoEcuA7t3pm+3nE/cCmsJM2NMYQZPv7ed3fsPUdA5s9X6FpHUaWqyWebuf44vMLN6n7VIeLIyItx5+Wiun/kWP/3Hcmpqna+feWyz+6utdV58dysPzf2A51dsAeDMYb2YeGKfWDLp0ZmB3TvTKy+bYNBFUpQWRZm9+hDPLt/c7od9i3QUTU02NwF/bkKZhCwzGuH2SScTsYX87MkV1LjzrbIhR9XHjn0HefSNtfzp9TWs2bGfnl2y+VbZECaPG0Dfro0/Vwnb4PwIxQWdeGrpJiUbkXaise/ZTAAuAPqa2e1xu/KJDUmWFMiIRvjVZScRjRg/f2olNTXOdWd/YlKEj3F33lyzk98uPsCCZ57jYE0t4wZ35/vjj+O8EUVkZaTPahNmxvkji3h43hr2VVWTm9360/aISHI19q94A/AGcCEfnwttL3BDWEFJ4zKiEX755ZOImvGLZ96hxp3vnDPsE/X2VVXz94XrefC1D1ixaS85GXD5uEF8ZdwAhhbm1dNzepgwqog/vPo+5Su38rkTilMdjoi0UGPfs1kELDKzPwV1B7j7yqREJo2KRoz/vvREIhHjV8++S22tc8O5wzAzVm7ayx/nfsDf3lpPZVU1I4rz+dmXjqfbnvcYf87IVIfeqNJB3emRm8VTSzcp2Yi0A029PzGe2CwCWcBgMzsJuLmR6WokCaIR4+cXn0BGxLj9+Qo27D7Amu37mff+DrIyInz++GL+5bSBnNy/K2ZGefmqVIfcJNGIcd7IQmYt3MCBQzXNGkYtIumjqcnmJ8TWkikHcPeFwfxkkgYiEeO/vng8kYjxp9fXMKB7Z354QQmXjOlP99ysVIfXbLHnNmt59b1tnFVSmOpwRKQFmppsDrn77jrDX/U9mzQSiRi3XDSKqz41iGN7dSESSd5Q5bB86tie5HXK4Mm3NynZiLRxTR2CtNTMLgeiZjbUzO4AXg0xLmkGM2NoYV67SDQQ+17ROcMLeWb5Zi2VLdLGNTXZXAeMBKqAh4E9wHfCCkrksPNHFrFr/yHmrd6R6lBEpAWaunjafuBHwUskac4c1ouczChPLtnEp4b0THU4ItJMjX2pc1ZD+zUaTcKWkxWl7LhezFm6if+8cGS7uUUo0tE0dmVzGrCW2K2z14mtlCmSVONHFfHkkk28tXYXYwZ2S3U4ItIMjT2zKQJ+CIwC/gc4F9jm7i+6+4thBycC8NmS3mRGjaeWbEx1KCLSTA0mG3evcfen3P1K4FSgAigPlmYWSYr8TpmcMaQnTy3dRBtY0VtE6tHoaDQzyzazLwF/BK4Fbgf+FnZgIvHGjypi7Y4PWbZxT6pDEZFmaDDZmNkDwGvAaOA/3f0Ud/8/7r6+KZ2b2XgzW2lmFWZ2Yz37s83skWD/62Y2KG7fTUH5SjM7v7E+zeyhoHyJmd1nZplBuZnZ7UH9xWY2uimxS3o5Z3ghEYM5SzalOhQRaYbGrmz+BRgKXA+8amZ7gtdeM2vwv5hmFgXuAiYAI4DJZjaiTrUpwE53HwLcBtwatB0BTCL23Z7xwN1mFm2kz4eAEuB4IAe4OiifEJzDUGAq8OtGzlnSUI8u2Ywb3IMnlWxE2qTGntlE3D0veOXHvfLcPb+RvscCFe6+yt0PAjOBiXXqTATuD7YfA8622Jw4E4GZ7l7l7quJPSsa21Cf7j7bA8A8oF/cMR4Ids0FupqZphFug8aPKuLdLZVUbKlMdSgicpTCXJWqL7Fh04etA8YlquPu1Wa2G+gRlM+t07ZvsN1gn8Hts38ldjWWKI6+wMY67aYSu/KhsLCQ8vLyxs6vXpWVlc1umwzpHh8kjjHvQGzKmt888SqfPzZ1E4ym+2eo+FpG8YWjPS6BeDfwkru/fDSN3H0GMAOgtLTUy8rKmnXw8vJymts2GdI9Pmg4xgfee4WV+53/V3ZGcoOKk+6foeJrGcUXjjDXAl4P9I973y8oq7eOmWUABcD2Bto22KeZ/RjoBfzbUcYhbcSEUUW8vX4363buT3UoInIUwkw284GhZjbYzLKIPfCvO/3NLODKYPsS4PngmcssYFIwWm0wsYf78xrq08yuBs4HJrt7bZ1jXBGMSjsV2O3u+nZgG3X+yCIA5izdnOJIRORohJZs3L0amAbMAZYDj7r7UjO72cwOz6l2L9DDzCqIXY3cGLRdCjwKLAOeAq4NvmBab59BX78BCoHXzGyhmU0PymcDq4gNMrgH+FZY5yzhG9Qzl5KiPA2BFmljQn1m4+6zif2yjy+bHrd9ALg0QdtbgFua0mdQXu+5BFdK1x5V4JLWJowq5lfPvcOWvQfondcp1eGISBOEeRtNJBTjRxXhDs8s0600kbZCyUbanGGFXRjcM5endCtNpM1QspE2x8w4f2QRr723nd37D6U6HBFpAiUbaZMmjCqiutZ5drlupYm0BUo20iad0K+A4oJOPLVUt9JE2gIlG2mTDt9Ke+mdreyrqk51OCLSCCUbabMmjCqiqrqW8pVbUx2KiDRCyUbarNJB3emRm6VbaSJtgJKNtFnRiHHeyEKeX76ZA4dqUh2OiDRAyUbatPNHFrHvYA2vVGxLdSgi0gAlG2nTPnVsT/I6ZegLniJpTslG2rSsjAjnDC/kmeWbqa6pbbyBiKSEko20eeePLGLX/kO8vnpHqkMRkQSUbKTNO3NYL3Iyo7qVJpLGlGykzcvJilJ2XC/mLN1Eba2nOhwRqUeo69mIJMv4UUU8uWQTb63dxZiB3Zrc7lBNLau27mPFpj2s2LSXLXuq+GbZMQzpnRditCIdj5KNtAufLelNZtR4asnGepONu7NlbxXLN+5h5aa9rNi0l+Ub9/De1koO1cSuhjKjRmY0wovvbOGhq0/luCIlHJHWomQj7UJ+p0zOGNKTp5Zu4oZzh/Hu5kpWbNrD8o17g+Syh51xyxEUF3SipCiPz5b0pqQoj5KifI7plcuaHfu5/J65TL5nLg9dPY7hxfkpPCuR9iPUZGNm44H/AaLA79z9/9bZnw08AIwBtgOXufv7wb6bgClADfBtd5/TUJ9mNg34DnAs0MvdtwXlZcDjwOrgsH9195tDOmVJofGjivjBX95m5I/n4MGjm85ZUY4rymP8qCJKivKPJJaCzpn19nFsry7MnHoak2d8lHBG9ilI4lmItE+hJRsziwJ3AecC64D5ZjbL3ZfFVZsC7HT3IWY2CbgVuMzMRgCTgJFAH+BZMxsWtEnU5yvA/wLl9YTzsrt/vtVPUtLK507ow9vrd9OrSyeOK8pjeHEe/bt1JhKxo+pncM9cHvn6qUyeMZfL73mdh64ex6i+SjgiLRHmaLSxQIW7r3L3g8BMYGKdOhOB+4Ptx4CzzcyC8pnuXuXuq4GKoL+Efbr7W4eviqRj6pKdwU8vOp7rzxnK+FFFDOyRe9SJ5rCBPXJ55Oun0SU7g8vvmcuitbtaOVqRjiXM22h9gbVx79cB4xLVcfdqM9sN9AjK59Zp2zfYbqzP+pxmZouADcD33H1p3QpmNhWYClBYWEh5eXkTuv2kysrKZrdNhnSPD9IrxhtOhFvn1TDpt6/w3dJODOkaTav46qP4WkbxhaMjDBB4Exjo7pVmdgHwd2Bo3UruPgOYAVBaWuplZWXNOlh5eTnNbZsM6R4fpF+Mp532IZffM5dfvXWQ+792MqxenFbx1ZVun19diq9l0j2+RMK8jbYe6B/3vl9QVm8dM8sACogNFEjUtil9foy773H3ymB7NpBpZj2P9mSk4+rbNYeZU0+lV142V9w7j3d2ajkDkaMVZrKZDww1s8FmlkXsgf+sOnVmAVcG25cAz7u7B+WTzCzbzAYTuxKZ18Q+P8bMioLnQJjZWGLnvL1VzlA6jOKCWMIpLOjEL944wNxV+hESORqhJRt3rwamAXOA5cCj7r7UzG42swuDavcCPcysAvg34Mag7VLgUWAZ8BRwrbvXJOoTwMy+bWbriF3tLDaz3wXHuARYEjyzuR2YFCQ0kaNSmN+JmdecSo9OxlW/n8+r72kNHZGmCvWZTXDbanadsulx2weASxO0vQW4pSl9BuW3E0smdcvvBO482thF6tM7vxM/GJvDXcsifO0P8/ndFadwxlDdlRVpjCbiFDlKBdnGw9ecyqAeuUy5fz4vvrM11SGJpD0lG5Fm6NElmz9dcyrH9OrCNQ+8wQsrtqQ6JJG0pmQj0kzdc7N4+JpxDCvswtcfXMBzyzenOiQtsSBpS8lGpAW6ds7ioSmnUlKcxzf+uICnlyZ3ATd3Z/nGPdz1QgVfuvsVhv7Hk/xm0QG27D2Q1DhEGtMRvtQpEqqCzpk8OGUcV9w3j2899CanHduDkwd0Y8zAbpzUvysFOfVP+tlcBw7V8Op723hu+RaeX7GFjbtjieWEfgVcPLovf12wjnN+8SI/mFDC5FMGNHvKHpHWpGQj0goKcjJ5cMpYfvn0O8xdtZ07n3+XWgczGNKrC2MGdmP0gG6MHtiVY3p2OeoEsGHXhzy/IpZcXn1vGwcO1dI5K8qnh/bkO+cM5bPH9aZ3ficATu60ncc35PCjvy3hLwvW8V9fOp6SIi2VIKmlZCPSSvI7ZfKTC0cCUFlVzaK1u1jwwU7eXLOTJ5dsYub82LR+BTmZnDygayz5DOjGSQO60iX74/8Ua2qdhWt38fyKzTy3fAsrNu0FoH/3HCadMoCzSnoz7pjuZGdEPxFHcZcID19zKn99cz0//ccyPn/7P5ny6cFcf/ZQOmfpn7ykhn7yRELQJTuD04f05PQhse/g1NY6q7bt480g+by5ZiflK2NDpiMGwwrzGD2wGyVFeSxcs4vyd7ayY99BohGjdGA3fnhBCWeV9ObYXl0IJsRokJlx8Zh+nFXSm589uZzfvriKfyzeyP+ZOIrPlvQO9dxF6qNkI5IEkYgxpHcXhvTuwpdPiU3vt/vDQyxcu+tIAnpi4Qb+VFVN186ZfPa43pxV0pvPDO2VcKG3puiWm8XPLzmRi0f344d/e5ur/u/A4/oAABFFSURBVDCfzx1fzPQvjKAwuO0mkgxKNiIpUpCTyZnDenHmsF5A7NbZhl0f0qdrDtFWfqg/7pgezL7+08x4cRV3vFDBS+9s5d/HH8dXxg1s9WOJ1EdDn0XSRDRi9O/eObRf/tkZUa47eyhPf+cznNi/K9MfX8qXfv0qSzfsDuV4IvGUbEQ6mEE9c3lwylj+Z9JJrN+5nwvvfIVb/rGMfVXVqQ5N2jElG5EOyMyYeFJfnvu3Mi47pT/3vLyac3/5Is8uS/0sCNI+KdmIdGAFnTP5ry8ez1++eRp5nTK5+oE3uO7ht9i1/2CqQ5N2RslGRBgzsDv/++0z+O65w3jy7Y2cd9tLmlxUWpWSjYgAkBmNcN3ZQ3l82ul065zFVX+Yz41/WUylnuVIK1CyEZGPGdmngFnXnc43zjyWR99Yy/hfvcRr72kZbGkZJRsR+YTsjCg3Tijhz984jYyIMfmeudz8xDIOHKpJdWjSRinZiEhCYwZ2Z/b1n+aK0wZy3yur+dztL7No7a5UhyVtUKjJxszGm9lKM6swsxvr2Z9tZo8E+183s0Fx+24Kylea2fmN9Wlm04IyN7OeceVmZrcH+xab2ejwzlik/emclcHNE0fxxynj2H+whi/9+lV+8fRKDlbXpjo0aUNCSzZmFgXuAiYAI4DJZjaiTrUpwE53HwLcBtwatB0BTAJGAuOBu80s2kifrwDnAB/UOcYEYGjwmgr8ujXPU6SjOGNoT576zme46KS+3PF8BV+8+xVWBrNRizQmzCubsUCFu69y94PATGBinToTgfuD7ceAsy02pe1EYKa7V7n7aqAi6C9hn+7+lru/X08cE4EHPGYu0NXMilv1TEU6iIKcTH7x5RP57b+OYfOeA3zhjn/ymxffo0bLUUsjwpyIsy+wNu79OmBcojruXm1mu4EeQfncOm37BtuN9dmUOPoCG+MrmdlUYlc+FBYWUl5e3ki39ausrGx222RI9/gg/WNUfJANTB+bwf1La/i/T67gsdfe4ZrjsynMbfz/r/r8Wibd40tEsz4H3H0GMAOgtLTUy8rKmtVPeXk5zW2bDOkeH6R/jIrvI18413l84QamP76En8w9yE0XlPCZob3IiBqZ0QjRiJEZiZARtdgrEuHll17U59cC6R5fImEmm/VA/7j3/YKy+uqsM7MMoADY3kjbxvpsThwi0gxmxkUn92XcMd35/mOLmf740sbbAJnPPElG1GLJKBohI/gzM2qMGdidz59QzOlDepKVoQGz7UWYyWY+MNTMBhP75T4JuLxOnVnAlcBrwCXA8+7uZjYL+JOZ/RLoQ+zh/jxiP6eN9VnXLGCamc0kdsttt7tvbKSNiByF4oIcHvjaWF56dxvbK6uornGqa53q2loO1TjVNbWx9zVOxerV9Os/gOqaYF9tLTW1zqEap/JANU8v28Rf3lxHfqcMzhtZxOeOV+JpD0JLNsEzmGnAHCAK3OfuS83sZuANd58F3As8aGYVwA5iyYOg3qPAMqAauNbdayA2xLlun0H5t4HvA0XAYjOb7e5XA7OBC4gNMtgPXBXWOYt0ZGZ2ZCG4hpSXr6esrCTh/qrqGv757jb+8fZG5izdxGMLlHjag1Cf2bj7bGK/7OPLpsdtHwAuTdD2FuCWpvQZlN8O3F5PuQPXHm3sIpIa2RlRzh5eyNnDC5V42hENEBCRtKXE034o2YhIm9CUxHPuiCKKCrJxBwc8+PqPEys4/G0gdz9Sh6CeEytbs66KJ7ct5lBNLQdrajkUPFs6VFPLweraj8qr/RN1at257JT+/Pt5x5ERVeKLp2QjIm1OosTz7PLNR5a3NgPDYsOKiP1xpOzIduxZ0+H9GFBTTeddW8iMRsjKiJAVjRwZKZcZjZCbnUHXuPeH92dlRNi6t4rfvriKRWt3ccfk0fTKy072R5O2lGxEpE2LTzytoaXfY/nLgnX88G9v8/k7Xubur4xmzMDurRJXW6frPBGRVnTxmH787Vunk50R5bLfzuX+V9/HXdP5KNmIiLSyEX3yeWLaGZw5rBc/nrWUGx5ZyP6DHXvFUyUbEZEQFHTO5J4rSvnuucN4fNEGvnjXq6zeti/VYaWMko2ISEgiEeO6s4fyh6vGsnnvAS684588vXRTqsNKCSUbEZGQnTmsF09MO4NBPXOZ+uACfv7Uig63LIOSjYhIEvTv3pk/f+M0Jo/tz93l73HlffPYXlmV6rCSRslGRCRJOmVG+dmXTuDWi49n3vs7+MId/2Th2l2pDisplGxERJLsslMG8JdvfIpIxPjyb17jj3M/aPfDo5VsRERS4Ph+BTwx7QxOO7YH//H3JXzvz4s5cKgm1WGFRjMIiIikSLfcLO776inc/ty7/M9z77J84x4mntTnE/O6xV/0vLfqIEtq3613/jfDOKF/AZ8e0jPt5mZTshERSaFoxLjh3GGc1L8r//boQn725IrGG737ToO7e+Rm8fkTirnwpL6MHtD1yPxvqaRkIyKSBj5b0pt5PzqHg9W1wEeThsbnCTN4+aWX+MxnzvzYRKKHqxyqreXFlVt5fNEGZs5fy/2vfUD/7jlMPLEvF53chyG985J+Xocp2YiIpInMYAbphmRELOG6PdmRKOeNLOK8kUXsPXCIOUs38/jC9dxdXsGdL1Qwojifi07uwxdO7ENxQU4Yp5A47qQeTUREkiKvUyaXjOnHJWP6sWXvAf6xeCN/X7iB/5q9gp89uYJxg7sz8aS+XDCqmILOmaHHo2QjItLO9c7rxFWnD+aq0wfz/rZ9PL5wA48vXM9Nf32b6Y8voey43lx0Ul/OHt6bTpnRUGIIdbiCmY03s5VmVmFmN9azP9vMHgn2v25mg+L23RSUrzSz8xvr08wGB31UBH1mBeVfNbOtZrYweF0d5jmLiKSzQT1zuf6coTz33TN5YtoZXHHaIBat3cW1f3qT0p8+y+9eXhXKcUO7sjGzKHAXcC6wDphvZrPcfVlctSnATncfYmaTgFuBy8xsBDAJGAn0AZ41s2FBm0R93grc5u4zzew3Qd+/Dto84u7TwjpXEZG2xsw4vl8Bx/cr4IcXDGfuqu08vnA9RQWdQjlemLfRxgIV7r4KwMxmAhOB+GQzEfhJsP0YcKfFxuhNBGa6exWw2swqgv6or08zWw6cBVwe1Lk/6PdwshERkQSiEeP0IT05fUjP0I4RZrLpC6yNe78OGJeojrtXm9luoEdQPrdO277Bdn199gB2uXt1PfUBLjazzwDvADe4e3wfAJjZVGAqQGFhIeXl5U07yzoqKyub3TYZ0j0+SP8YFV/LKL6WSff4EukIAwSeAB529yoz+zqxq56z6lZy9xnADIDS0lJv7hrkLV2/PGzpHh+kf4yKr2UUX8uke3yJhDlAYD3QP+59v6Cs3jpmlgEUANsbaJuofDvQNejjY8dy9+3B7TiA3wFjWnRWIiJy1MJMNvOBocEosSxiD/xn1akzC7gy2L4EeN5jU5/OAiYFo9UGA0OBeYn6DNq8EPRB0OfjAGZWHHe8C4HlrXyeIiLSiNBuowXPYKYBc4AocJ+7LzWzm4E33H0WcC/wYDAAYAex5EFQ71FigwmqgWvdvQagvj6DQ/4AmGlmPwXeCvoG+LaZXRj0swP4aljnLCIi9Qv1mY27zwZm1ymbHrd9ALg0QdtbgFua0mdQvoqPRqzFl98E3HS0sYuISOtJrzmoRUSkXVKyERGR0Fl7X4q0OcxsK/BBM5v3BLa1YjitLd3jg/SPUfG1jOJrmXSOb6C796pvh5JNKzOzN9y9NNVxJJLu8UH6x6j4WkbxtUy6x5eIbqOJiEjolGxERCR0Sjatb0aqA2hEuscH6R+j4msZxdcy6R5fvfTMRkREQqcrGxERCZ2SjYiIhE7JpplasuR1EmLrb2YvmNkyM1tqZtfXU6fMzHbHLZc9vb6+QozxfTN7Ozj2G/XsNzO7Pfj8FpvZ6CTGdlzc57LQzPaY2Xfq1En652dm95nZFjNbElfW3cyeMbN3gz+7JWh7ZVDnXTO7sr46IcX332a2Ivg7/JuZdU3QtsGfhxDj+4mZrY/7e7wgQdsG/72HGN8jcbG9b2YLE7QN/fNrMXfX6yhfxCYBfQ84BsgCFgEj6tT5FvCbYHsSsaWpkxVfMTA62M4jtmhc3fjKgP9N4Wf4PtCzgf0XAE8CBpwKvJ7Cv+tNxL6sltLPD/gMMBpYElf2c+DGYPtG4NZ62nUHVgV/dgu2uyUpvvOAjGD71vria8rPQ4jx/QT4XhN+Bhr89x5WfHX2/wKYnqrPr6UvXdk0z5Elr939IHB4yet4E4kt1AaxJa/PDpa8Dp27b3T3N4PtvcSWVejbcKu0MxF4wGPmEluvqLixRiE4G3jP3Zs7o0SrcfeXiM1cHi/+5+x+4KJ6mp4PPOPuO9x9J/AMMD4Z8bn70/7RCrpzia01lRIJPr+maMq/9xZrKL7gd8eXgYdb+7jJomTTPPUteV33l/nHlrwGDi95nVTB7buTgdfr2X2amS0ysyfNbGRSAwMHnjazBRZbkruupnzGyTCJxP/AU/n5HVbo7huD7U1AYT110uWz/Bqxq9X6NPbzEKZpwW2++xLchkyHz+/TwGZ3fzfB/lR+fk2iZNOOmVkX4C/Ad9x9T53dbxK7NXQicAfw9ySHd4a7jwYmANea2WeSfPxGWWyBvguBP9ezO9Wf3yd47H5KWn6Xwcx+RGxNqYcSVEnVz8OvgWOBk4CNxG5VpaPJNHxVk/b/npRsmqclS14nhZllEks0D7n7X+vud/c97l4ZbM8GMs2sZ7Lic/fDy3ZvAf7GJ9ciaspnHLYJwJvuvrnujlR/fnE2H769GPy5pZ46Kf0szeyrwOeBrwQJ8ROa8PMQCnff7O417l4L3JPguKn+/DKALwGPJKqTqs/vaCjZNE9LlrwOXXB/915gubv/MkGdosPPkMxsLLGfhaQkQzPLNbO8w9vEHiIvqVNtFnBFMCrtVGB33O2iZEn4v8lUfn51xP+cHVkOvY45wHlm1i24TXReUBY6MxsPfB+40N33J6jTlJ+HsOKLfw74xQTHbcq/9zCdA6xw93X17Uzl53dUUj1Coa2+iI2WeofYKJUfBWU3E/tHBdCJ2O2XCmAecEwSYzuD2O2UxcDC4HUB8A3gG0GdacBSYiNr5gKfSmJ8xwTHXRTEcPjzi4/PgLuCz/dtoDTJf7+5xJJHQVxZSj8/YolvI3CI2HODKcSeAz4HvAs8C3QP6pYCv4tr+7XgZ7ECuCqJ8VUQe95x+Ofw8AjNPsDshn4ekhTfg8HP12JiCaS4bnzB+0/8e09GfEH5Hw7/3MXVTfrn19KXpqsREZHQ6TaaiIiETslGRERCp2QjIiKhU7IREZHQKdmIiEjolGxEUsDMaurMLN1qMwmb2aD4mYNF0kFGqgMQ6aA+dPeTUh2ESLLoykYkjQTrkvw8WJtknpkNCcoHmdnzwYSRz5nZgKC8MFgnZlHw+lTQVdTM7rHYekZPm1lOyk5KBCUbkVTJqXMb7bK4fbvd/XjgTuBXQdkdwP3ufgKxySxvD8pvB1702ISgo4l9gxxgKHCXu48EdgEXh3w+Ig3SDAIiKWBmle7epZ7y94Gz3H1VMJnqJnfvYWbbiE2lcigo3+juPc1sK9DP3avi+hhEbP2aocH7HwCZ7v7T8M9MpH66shFJP55g+2hUxW3XoOezkmJKNiLp57K4P18Ltl8lNtswwFeAl4Pt54BvAphZ1MwKkhWkyNHQ/3ZEUiPHzBbGvX/K3Q8Pf+5mZouJXZ1MDsquA35vZv8ObAWuCsqvB2aY2RRiVzDfJDZzsEha0TMbkTQSPLMpdfdtqY5FpDXpNpqIiIROVzYiIhI6XdmIiEjolGxERCR0SjYiIhI6JRsREQmdko2IiITu/wdLLz7gDInLuAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH 19\n",
      "The student reached the baseline\n"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "seed_all(112)\n",
    "\n",
    "timer = Timer('student')\n",
    "for n in range(StudentConfig.num_epochs):\n",
    "    with timer:\n",
    "        dkd_train_epoch(teacher, student, sopt, train_loader, melspec_train, sconfig.device, sconfig.T, sconfig.a)\n",
    "\n",
    "        au_fa_fr = dkd_validation(teacher, student, val_loader,\n",
    "                          melspec_val, sconfig.device, sconfig.T, sconfig.a)\n",
    "        history['val_metric'].append(au_fa_fr)\n",
    "\n",
    "    clear_output()\n",
    "    print(f\"{timer.name.capitalize()} | Elapsed time : {timer.total_time:.2f}\")\n",
    "    plt.plot(history['val_metric'])\n",
    "    plt.ylabel('Metric')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.grid()\n",
    "    plt.savefig('mix_pres_student.png')\n",
    "    plt.show()\n",
    "\n",
    "    print('END OF EPOCH', n)\n",
    "    if au_fa_fr < 5e-5 * 1.1:\n",
    "        print('The student reached the baseline')\n",
    "        torch.save(student, 'student_model.pth')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hinIFRQbI1K",
    "outputId": "da65905b-edec-4fc6-8009-98bf90dcd6af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'val_metric': [0.00041010180486984705,\n",
       "              0.0002505027285865371,\n",
       "              0.00022199277001772806,\n",
       "              0.0002338920598944848,\n",
       "              0.00023785152771455906,\n",
       "              0.0002443979274110496,\n",
       "              0.00015446698536354526,\n",
       "              0.00015963189779500607,\n",
       "              0.0001311338743213593,\n",
       "              0.00012466505274342336,\n",
       "              0.00013321356465338023,\n",
       "              0.00011520947360113598,\n",
       "              9.547479375036646e-05,\n",
       "              9.494368201564662e-05,\n",
       "              8.530609267213505e-05,\n",
       "              8.538665456448017e-05,\n",
       "              8.60788900838903e-05,\n",
       "              6.353349532241243e-05,\n",
       "              6.361405721475757e-05,\n",
       "              5.339761575587717e-05]})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZL2K1b1irnk"
   },
   "source": [
    "## FLOPs/MACs estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zEz4u-BPbMVk",
    "outputId": "4172368c-0ed4-4092-8d9f-9757f451b638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: thop in /usr/local/lib/python3.7/dist-packages (0.1.1.post2209072238)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from thop) (1.12.1+cu113)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->thop) (4.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install thop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7KOJipUbOfU",
    "outputId": "074b36a0-1cda-4e06-c5f8-8ec5c1da77ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "(1949056.0, 70443.0)\n",
      "\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "(189104.0, 4737.0)\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "\n",
    "list_audio = [train_set[0]['wav'], train_set[1]['wav']]\n",
    "audio = torch.cat(list_audio)\n",
    "mel = melspec_train(audio.unsqueeze(0).to(tconfig.device))\n",
    "\n",
    "print(profile(teacher, (mel, ) ))\n",
    "print('')\n",
    "student.half()\n",
    "print(profile(student, (mel.half(), ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jx9aLj3sbSFV",
    "outputId": "2930c695-d7f7-4e18-c031-b001d23d59dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.30679414502073 = teacher macs / student macs\n",
      "14.870804306523116 = teacher params / student params\n"
     ]
    }
   ],
   "source": [
    "print(1949056.0 / 189104.0, \"= teacher macs / student macs\")\n",
    "print(70443.0 / 4737.0, \"= teacher params / student params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lV7JP38jFte"
   },
   "source": [
    "## Memory estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KxLPu5YkbXFj",
    "outputId": "b996fbde-3092-4b03-f78c-790e2913eb37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.222162812458226 = teacher memory / student memory\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "def get_size_in_megabytes(model):\n",
    "    # https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html#look-at-model-size\n",
    "    with tempfile.TemporaryFile() as f:\n",
    "        torch.save(model.state_dict(), f)\n",
    "        size = f.tell() / 2**20\n",
    "    return size\n",
    "\n",
    "print(get_size_in_megabytes(teacher) / get_size_in_megabytes(student), \"= teacher memory / student memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZ-t4MqTUUwI",
    "outputId": "92da809d-7522-4de4-900a-8c36ba5db48d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv.0.weight: torch.float16\n",
      "conv.0.bias: torch.float16\n",
      "gru.weight_ih_l0: torch.float16\n",
      "gru.weight_hh_l0: torch.float16\n",
      "gru.bias_ih_l0: torch.float16\n",
      "gru.bias_hh_l0: torch.float16\n",
      "attention.energy.0.weight: torch.float16\n",
      "attention.energy.0.bias: torch.float16\n",
      "attention.energy.2.weight: torch.float16\n",
      "attention.energy.2.bias: torch.float16\n",
      "classifier.weight: torch.float16\n",
      "classifier.bias: torch.float16\n"
     ]
    }
   ],
   "source": [
    "for name, para in student.named_parameters():\n",
    "    print('{}: {}'.format(name, para.dtype))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
