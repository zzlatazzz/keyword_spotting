{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lhrn5O-qUYZ"
   },
   "source": [
    "# Import and misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "meO-Mp9jiAFC"
   },
   "outputs": [],
   "source": [
    "# Instal latest torch and torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bbUpoArCqUYa"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Union, List, Callable, Optional\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import pathlib\n",
    "import dataclasses\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torchaudio\n",
    "from IPython import display as display_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "812GwLfqqUYf"
   },
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8PdhApeEh9pH"
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class TaskConfig:\n",
    "    keyword: str = 'sheila'  # We will use 1 key word -- 'sheila'\n",
    "    batch_size: int = 128\n",
    "    learning_rate: float = 3e-4\n",
    "    weight_decay: float = 1e-5\n",
    "    num_epochs: int = 20\n",
    "    n_mels: int = 40\n",
    "    cnn_out_channels: int = 8\n",
    "    kernel_size: Tuple[int, int] = (5, 20)\n",
    "    stride: Tuple[int, int] = (2, 8)\n",
    "    hidden_size: int = 64\n",
    "    gru_num_layers: int = 2\n",
    "    bidirectional: bool = False\n",
    "    num_classes: int = 2\n",
    "    sample_rate: int = 16000\n",
    "    device: torch.device = torch.device(\n",
    "        'cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KA1gPmE1h9pI"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y2N8zcx9MF1X",
    "outputId": "28c05a78-9030-4408-b5ae-f67b16d03fcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-03 22:39:46--  http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\n",
      "Resolving download.tensorflow.org (download.tensorflow.org)... 108.177.119.128, 2a00:1450:4013:c00::80\n",
      "Connecting to download.tensorflow.org (download.tensorflow.org)|108.177.119.128|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1489096277 (1.4G) [application/gzip]\n",
      "Saving to: ‘speech_commands_v0.01.tar.gz’\n",
      "\n",
      "speech_commands_v0. 100%[===================>]   1.39G  94.2MB/s    in 16s     \n",
      "\n",
      "2022-11-03 22:40:02 (89.0 MB/s) - ‘speech_commands_v0.01.tar.gz’ saved [1489096277/1489096277]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz -O speech_commands_v0.01.tar.gz\n",
    "!mkdir speech_commands && tar -C speech_commands -xvzf speech_commands_v0.01.tar.gz 1> log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "12wBTK0mNUsG"
   },
   "outputs": [],
   "source": [
    "class SpeechCommandDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform: Optional[Callable] = None,\n",
    "        path2dir: str = None,\n",
    "        keywords: Union[str, List[str]] = None,\n",
    "        csv: Optional[pd.DataFrame] = None\n",
    "    ):        \n",
    "        self.transform = transform\n",
    "\n",
    "        if csv is None:\n",
    "            path2dir = pathlib.Path(path2dir)\n",
    "            keywords = keywords if isinstance(keywords, list) else [keywords]\n",
    "            \n",
    "            all_keywords = [\n",
    "                p.stem for p in path2dir.glob('*')\n",
    "                if p.is_dir() and not p.stem.startswith('_')\n",
    "            ]\n",
    "\n",
    "            triplets = []\n",
    "            for keyword in all_keywords:\n",
    "                paths = (path2dir / keyword).rglob('*.wav')\n",
    "                if keyword in keywords:\n",
    "                    for path2wav in paths:\n",
    "                        triplets.append((path2wav.as_posix(), keyword, 1))\n",
    "                else:\n",
    "                    for path2wav in paths:\n",
    "                        triplets.append((path2wav.as_posix(), keyword, 0))\n",
    "            \n",
    "            self.csv = pd.DataFrame(\n",
    "                triplets,\n",
    "                columns=['path', 'keyword', 'label']\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.csv = csv\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        instance = self.csv.iloc[index]\n",
    "\n",
    "        path2wav = instance['path']\n",
    "        wav, sr = torchaudio.load(path2wav)\n",
    "        wav = wav.sum(dim=0)\n",
    "        \n",
    "        if self.transform:\n",
    "            wav = self.transform(wav)\n",
    "\n",
    "        return {\n",
    "            'wav': wav,\n",
    "            'keywors': instance['keyword'],\n",
    "            'label': instance['label']\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-1rVkT81Pk90"
   },
   "outputs": [],
   "source": [
    "dataset = SpeechCommandDataset(\n",
    "    path2dir='speech_commands', keywords=TaskConfig.keyword\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "DFwhAXdfQLIA",
    "outputId": "dcdad629-8c94-4f8a-af61-27cf91ec11fe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-2aac79fa-2375-4e05-9121-fe0a668a813c\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>keyword</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47265</th>\n",
       "      <td>speech_commands/three/b5552931_nohash_3.wav</td>\n",
       "      <td>three</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17969</th>\n",
       "      <td>speech_commands/two/333784b7_nohash_4.wav</td>\n",
       "      <td>two</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37669</th>\n",
       "      <td>speech_commands/seven/8eb4a1bf_nohash_1.wav</td>\n",
       "      <td>seven</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14805</th>\n",
       "      <td>speech_commands/yes/8a0457c9_nohash_0.wav</td>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14371</th>\n",
       "      <td>speech_commands/yes/60472d26_nohash_1.wav</td>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2aac79fa-2375-4e05-9121-fe0a668a813c')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-2aac79fa-2375-4e05-9121-fe0a668a813c button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-2aac79fa-2375-4e05-9121-fe0a668a813c');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                              path keyword  label\n",
       "47265  speech_commands/three/b5552931_nohash_3.wav   three      0\n",
       "17969    speech_commands/two/333784b7_nohash_4.wav     two      0\n",
       "37669  speech_commands/seven/8eb4a1bf_nohash_1.wav   seven      0\n",
       "14805    speech_commands/yes/8a0457c9_nohash_0.wav     yes      0\n",
       "14371    speech_commands/yes/60472d26_nohash_1.wav     yes      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.csv.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUxfDJw1qUYi"
   },
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dkmkxPWQqUYe"
   },
   "outputs": [],
   "source": [
    "class AugsCreation:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.background_noises = [\n",
    "            'speech_commands/_background_noise_/white_noise.wav',\n",
    "            'speech_commands/_background_noise_/dude_miaowing.wav',\n",
    "            'speech_commands/_background_noise_/doing_the_dishes.wav',\n",
    "            'speech_commands/_background_noise_/exercise_bike.wav',\n",
    "            'speech_commands/_background_noise_/pink_noise.wav',\n",
    "            'speech_commands/_background_noise_/running_tap.wav'\n",
    "        ]\n",
    "\n",
    "        self.noises = [\n",
    "            torchaudio.load(p)[0].squeeze()\n",
    "            for p in self.background_noises\n",
    "        ]\n",
    "\n",
    "    def add_rand_noise(self, audio):\n",
    "\n",
    "        # randomly choose noise\n",
    "        noise_num = torch.randint(low=0, high=len(\n",
    "            self.background_noises), size=(1,)).item()\n",
    "        noise = self.noises[noise_num]\n",
    "\n",
    "        noise_level = torch.Tensor([1])  # [0, 40]\n",
    "\n",
    "        noise_energy = torch.norm(noise)\n",
    "        audio_energy = torch.norm(audio)\n",
    "        alpha = (audio_energy / noise_energy) * \\\n",
    "            torch.pow(10, -noise_level / 20)\n",
    "\n",
    "        start = torch.randint(\n",
    "            low=0,\n",
    "            high=max(int(noise.size(0) - audio.size(0) - 1), 1),\n",
    "            size=(1,)\n",
    "        ).item()\n",
    "        noise_sample = noise[start: start + audio.size(0)]\n",
    "\n",
    "        audio_new = audio + alpha * noise_sample\n",
    "        audio_new.clamp_(-1, 1)\n",
    "        return audio_new\n",
    "\n",
    "    def __call__(self, wav):\n",
    "        aug_num = torch.randint(low=0, high=4, size=(1,)).item()   # choose 1 random aug from augs\n",
    "        augs = [\n",
    "            lambda x: x,\n",
    "            lambda x: (x + distributions.Normal(0, 0.01).sample(x.size())).clamp_(-1, 1),\n",
    "            lambda x: torchaudio.transforms.Vol(.25)(x),\n",
    "            lambda x: self.add_rand_noise(x)\n",
    "        ]\n",
    "\n",
    "        return augs[aug_num](wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ClWThxyYh9pM"
   },
   "outputs": [],
   "source": [
    "indexes = torch.randperm(len(dataset))\n",
    "train_indexes = indexes[:int(len(dataset) * 0.8)]\n",
    "val_indexes = indexes[int(len(dataset) * 0.8):]\n",
    "\n",
    "train_df = dataset.csv.iloc[train_indexes].reset_index(drop=True)\n",
    "val_df = dataset.csv.iloc[val_indexes].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PDPLht5fqUYe"
   },
   "outputs": [],
   "source": [
    "# Sample is a dict of utt, word and label\n",
    "train_set = SpeechCommandDataset(csv=train_df, transform=AugsCreation())\n",
    "val_set = SpeechCommandDataset(csv=val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mmrJd8WIhkLP",
    "outputId": "630bb975-93f4-4908-8b2f-5aa351d3db87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wav': tensor([-3.8147e-05, -3.0518e-05, -1.5259e-05,  ..., -1.5259e-05,\n",
       "         -2.2888e-05, -5.3406e-05]), 'keywors': 'one', 'label': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vbPDqd6qUYj"
   },
   "source": [
    "### Sampler for oversampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rfnjRKo2qUYj"
   },
   "outputs": [],
   "source": [
    "# We should provide to WeightedRandomSampler _weight for every sample_; by default it is 1/len(target)\n",
    "\n",
    "def get_sampler(target):\n",
    "    class_sample_count = np.array(\n",
    "        [len(np.where(target == t)[0]) for t in np.unique(target)])   # for every class count it's number of occ.\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in target])\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    samples_weigth = samples_weight.float()\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UM8gLmHeqUYj"
   },
   "outputs": [],
   "source": [
    "train_sampler = get_sampler(train_set.csv['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lyBqbxp0h9pO"
   },
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        wavs = []\n",
    "        labels = []    \n",
    "\n",
    "        for el in data:\n",
    "            wavs.append(el['wav'])\n",
    "            labels.append(el['label'])\n",
    "\n",
    "        # torch.nn.utils.rnn.pad_sequence takes list(Tensors) and returns padded (with 0.0) Tensor\n",
    "        wavs = pad_sequence(wavs, batch_first=True)    \n",
    "        labels = torch.Tensor(labels).long()\n",
    "        return wavs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8G9xPRVqUYk"
   },
   "source": [
    "###  Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6wGBMcQiqUYk"
   },
   "outputs": [],
   "source": [
    "# Here we are obliged to use shuffle=False because of our sampler with randomness inside.\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=TaskConfig.batch_size,\n",
    "                          shuffle=False, collate_fn=Collator(),\n",
    "                          sampler=train_sampler,\n",
    "                          num_workers=2, pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(val_set, batch_size=TaskConfig.batch_size,\n",
    "                        shuffle=False, collate_fn=Collator(),\n",
    "                        num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTlsn6cpqUYk"
   },
   "source": [
    "### Creating MelSpecs on GPU for speeeed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "pRXMt6it56fW"
   },
   "outputs": [],
   "source": [
    "class LogMelspec:\n",
    "\n",
    "    def __init__(self, is_train, config):\n",
    "        # with augmentations\n",
    "        if is_train:\n",
    "            self.melspec = nn.Sequential(\n",
    "                torchaudio.transforms.MelSpectrogram(\n",
    "                    sample_rate=config.sample_rate,\n",
    "                    n_fft=400,\n",
    "                    win_length=400,\n",
    "                    hop_length=160,\n",
    "                    n_mels=config.n_mels\n",
    "                ),\n",
    "                torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "                torchaudio.transforms.TimeMasking(time_mask_param=35),\n",
    "            ).to(config.device)\n",
    "\n",
    "        # no augmentations\n",
    "        else:\n",
    "            self.melspec = torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=config.sample_rate,\n",
    "                n_fft=400,\n",
    "                win_length=400,\n",
    "                hop_length=160,\n",
    "                n_mels=config.n_mels\n",
    "            ).to(config.device)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # already on device\n",
    "        return torch.log(self.melspec(batch).clamp_(min=1e-9, max=1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Pqkz4_gn8BiF"
   },
   "outputs": [],
   "source": [
    "melspec_train = LogMelspec(is_train=True, config=TaskConfig)\n",
    "melspec_val = LogMelspec(is_train=False, config=TaskConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoAxmihY8yxr"
   },
   "source": [
    "### Quality measurment functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "euwD1UyuqUYk"
   },
   "outputs": [],
   "source": [
    "# FA - true: 0, model: 1\n",
    "# FR - true: 1, model: 0\n",
    "\n",
    "def count_FA_FR(preds, labels):\n",
    "    FA = torch.sum(preds[labels == 0])\n",
    "    FR = torch.sum(labels[preds == 0])\n",
    "    \n",
    "    # torch.numel - returns total number of elements in tensor\n",
    "    return FA.item() / torch.numel(preds), FR.item() / torch.numel(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "YHBUrkT1qUYk"
   },
   "outputs": [],
   "source": [
    "def get_au_fa_fr(probs, labels):\n",
    "    sorted_probs, _ = torch.sort(probs)\n",
    "    sorted_probs = torch.cat((torch.Tensor([0]), sorted_probs, torch.Tensor([1])))\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "        \n",
    "    FAs, FRs = [], []\n",
    "    for prob in sorted_probs:\n",
    "        preds = (probs >= prob) * 1\n",
    "        FA, FR = count_FA_FR(preds, labels)        \n",
    "        FAs.append(FA)\n",
    "        FRs.append(FR)\n",
    "    # plt.plot(FAs, FRs)\n",
    "    # plt.show()\n",
    "\n",
    "    # ~ area under curve using trapezoidal rule\n",
    "    return -np.trapz(FRs, x=FAs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcEP5cEZqUYl"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cP_pFIsy5p2",
    "outputId": "05af8f8e-6c12-407f-c4e4-0382ee80cffa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRNN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(5, 20), stride=(2, 8))\n",
       "    (1): Flatten(start_dim=1, end_dim=2)\n",
       "  )\n",
       "  (gru): GRU(144, 64, num_layers=2, batch_first=True, dropout=0.1)\n",
       "  (attention): Attention(\n",
       "    (energy): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.energy = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        energy = self.energy(input)\n",
    "        alpha = torch.softmax(energy, dim=-2)\n",
    "        return (input * alpha).sum(dim=-2)\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = Attention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return output\n",
    "\n",
    "config = TaskConfig()\n",
    "model = CRNN(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "PpyvKwp0k3IU"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "vt2kjqC-IobK"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Uz2hRvhiuzoZ"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Timer:\n",
    "\n",
    "    def __init__(self, name: str, verbose=False):\n",
    "        self.name = name\n",
    "        self.verbose = verbose\n",
    "        self.total_time = 0\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.t = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.t = time.time() - self.t\n",
    "        self.total_time += self.t\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"{self.name.capitalize()} | Elapsed time : {self.t:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SmkSRvg36KdC",
    "outputId": "df697b73-a30d-46d4-bafe-73710c26db27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRNN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(5, 20), stride=(2, 8))\n",
       "    (1): Flatten(start_dim=1, end_dim=2)\n",
       "  )\n",
       "  (gru): GRU(144, 64, num_layers=2, batch_first=True, dropout=0.1)\n",
       "  (attention): Attention(\n",
       "    (energy): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tconfig = TaskConfig()\n",
    "teacher = torch.load('teacher_model.pth', map_location=tconfig.device)\n",
    "teacher.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPMgfwowi3X-"
   },
   "source": [
    "## Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "uNGzqsTVrQwW"
   },
   "outputs": [],
   "source": [
    "class SmallAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.energy = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        energy = self.energy(input)\n",
    "        alpha = torch.softmax(energy, dim=-2)\n",
    "        return (input * alpha).sum(dim=-2)\n",
    "\n",
    "class SmallCRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TaskConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=config.cnn_out_channels,\n",
    "                kernel_size=config.kernel_size, stride=config.stride\n",
    "            ),\n",
    "            nn.Flatten(start_dim=1, end_dim=2),\n",
    "        )\n",
    "\n",
    "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
    "            config.stride[0] + 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.gru_num_layers,\n",
    "            dropout=0.1,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.attention = SmallAttention(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = input.unsqueeze(dim=1)\n",
    "        conv_output = self.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = self.gru(conv_output)\n",
    "        contex_vector = self.attention(gru_output)\n",
    "        output = self.classifier(contex_vector)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "i-7dPBrq6KfH"
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class StudentConfig:\n",
    "    keyword: str = 'sheila'  # We will use 1 key word -- 'sheila'\n",
    "    batch_size: int = 128\n",
    "    learning_rate: float = 1e-3 # 3e-4\n",
    "    weight_decay: float = 1e-5 # 1e-5\n",
    "    num_epochs: int = 40 # 20\n",
    "    n_mels: int = 40\n",
    "    cnn_out_channels: int = 2 # 8\n",
    "    kernel_size: Tuple[int, int] = (5, 20) # (5, 20)\n",
    "    stride: Tuple[int, int] = (2, 8) # (2, 8)\n",
    "    hidden_size: int = 24 # 64\n",
    "    gru_num_layers: int = 1 # 2\n",
    "    bidirectional: bool = False\n",
    "    num_classes: int = 2\n",
    "    sample_rate: int = 16000\n",
    "    device: torch.device = torch.device(\n",
    "        'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    T: float = 10\n",
    "    a: float = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QzVrM_1w6Kjc",
    "outputId": "febbc8f1-7cb6-4d9f-f092-11deb0b86e41"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "sconfig = StudentConfig()\n",
    "student = SmallCRNN(sconfig).to(sconfig.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lV7JP38jFte"
   },
   "source": [
    "## Memory estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m5UDTGrujEut",
    "outputId": "a30f7ab5-c028-48fb-ea16-bb649f1fab80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.944058484311856 = teacher memory / student memory\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "def get_size_in_megabytes(model):\n",
    "    # https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html#look-at-model-size\n",
    "    with tempfile.TemporaryFile() as f:\n",
    "        torch.save(model.state_dict(), f)\n",
    "        size = f.tell() / 2**20\n",
    "    return size\n",
    "\n",
    "print(get_size_in_megabytes(teacher) / get_size_in_megabytes(student), \"= teacher memory / student memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZL2K1b1irnk"
   },
   "source": [
    "## FLOPs/MACs estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EiZD0qFi7GZP",
    "outputId": "0085b6df-b532-4605-acc9-a9d78ec42e6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting thop\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from thop) (1.12.1+cu113)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->thop) (4.1.1)\n",
      "Installing collected packages: thop\n",
      "Successfully installed thop-0.1.1.post2209072238\n"
     ]
    }
   ],
   "source": [
    "!pip install thop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jc5B0pQN7Ne8",
    "outputId": "d1c11049-9552-4c20-a67f-41aff4a20bca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "(1949056.0, 70443.0)\n",
      "\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_gru() for <class 'torch.nn.modules.rnn.GRU'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "(189936.0, 4741.0)\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "\n",
    "list_audio = [train_set[0]['wav'], train_set[1]['wav']]\n",
    "audio = torch.cat(list_audio)\n",
    "mel = melspec_train(audio.unsqueeze(0).to(tconfig.device))\n",
    "\n",
    "print(profile(teacher, (mel, ) ))\n",
    "print('')\n",
    "print(profile(student, (mel, ) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0VAeBf9NPXWT",
    "outputId": "5cd135cf-5d62-4c38-dcff-533f2847a42a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.261646028135793 = teacher macs / student macs\n",
      "14.858257751529214 = teacher params / student params\n"
     ]
    }
   ],
   "source": [
    "print(1949056.0 / 189936.0, \"= teacher macs / student macs\")\n",
    "print(70443.0 / 4741.0, \"= teacher params / student params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIQrRXlskAOY"
   },
   "source": [
    "# Student training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvlzuXugsTZp"
   },
   "outputs": [],
   "source": [
    "def dkd_train_epoch(teacher, student, opt, loader, log_melspec, device, T, a):\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "\n",
    "    for i, (batch, labels) in tqdm(enumerate(loader), total=len(loader)):\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        batch = log_melspec(batch)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        input = batch.unsqueeze(dim=1)\n",
    "        conv_output = student.conv(input).transpose(-1, -2)\n",
    "        gru_output, _ = student.gru(conv_output)\n",
    "        contex_vector = student.attention(gru_output)\n",
    "        logits_student = student.classifier(contex_vector)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_ = batch.unsqueeze(dim=1)\n",
    "            conv_output_ = teacher.conv(input_).transpose(-1, -2)\n",
    "            gru_output_, _ = teacher.gru(conv_output_)\n",
    "            contex_vector_ = teacher.attention(gru_output_)\n",
    "            logits_teacher = teacher.classifier(contex_vector_)\n",
    "\n",
    "        # distillation loss + student loss\n",
    "        probs_student = F.log_softmax(logits_student / T, dim=-1)\n",
    "        probs_teacher = F.log_softmax(logits_teacher / T, dim=-1)\n",
    "  \n",
    "        probs = F.softmax(logits_student, dim=-1)\n",
    "        #print(contex_vector.shape, contex_vector_.shape)\n",
    "        loss = nn.KLDivLoss()(probs_student, probs_teacher) * (T ** 2) * a + F.cross_entropy(logits_student, labels) * (1 - a)# + nn.KLDivLoss()(contex_vector, contex_vector_) * 0.1\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(student.parameters(), 5)\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        # logging\n",
    "        argmax_probs = torch.argmax(probs, dim=-1)\n",
    "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
    "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKgIipFzwDYF"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def dkd_validation(teacher, student, loader, log_melspec, device, T, a):\n",
    "    model.eval()\n",
    "    teacher.eval()\n",
    "    \n",
    "    val_losses, accs, FAs, FRs = [], [], [], []\n",
    "    all_probs, all_labels = [], []\n",
    "    for i, (batch, labels) in tqdm(enumerate(loader)):\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        batch = log_melspec(batch)\n",
    "\n",
    "        logits_student = student(batch)\n",
    "        logits_teacher = teacher(batch)\n",
    "\n",
    "        # distillation loss + student loss\n",
    "        probs_student = F.log_softmax(logits_student / T, dim=-1)\n",
    "        probs_teacher = F.log_softmax(logits_teacher / T, dim=-1)\n",
    "        probs = F.softmax(logits_student, dim=-1)\n",
    "        loss = nn.KLDivLoss()(probs_student, probs_teacher) * (T ** 2) * a + F.cross_entropy(logits_student, labels) * (1 - a)\n",
    "\n",
    "        # logging\n",
    "        argmax_probs = torch.argmax(probs, dim=-1)\n",
    "        all_probs.append(probs[:, 1].cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "        val_losses.append(loss.item())\n",
    "        accs.append(\n",
    "            torch.sum(argmax_probs == labels).item() /  # ???\n",
    "            torch.numel(argmax_probs)\n",
    "        )\n",
    "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
    "        FAs.append(FA)\n",
    "        FRs.append(FR)\n",
    "\n",
    "    # area under FA/FR curve for whole loader\n",
    "    au_fa_fr = get_au_fa_fr(torch.cat(all_probs, dim=0).cpu(), all_labels)\n",
    "    return au_fa_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CleyHrInxZur"
   },
   "outputs": [],
   "source": [
    "sopt = torch.optim.Adam(\n",
    "    student.parameters(),\n",
    "    lr=sconfig.learning_rate,\n",
    "    weight_decay=sconfig.weight_decay\n",
    ")\n",
    "\n",
    "history = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "MsY9uiXqxZ1X",
    "outputId": "b91f19c1-5df8-4652-eb0f-65a5c22fe7a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student | Elapsed time : 1193.88\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF EPOCH 25\n",
      "The student reached the baseline\n"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "seed_all(112)\n",
    "\n",
    "timer = Timer('student')\n",
    "for n in range(StudentConfig.num_epochs):\n",
    "    with timer:\n",
    "        dkd_train_epoch(teacher, student, sopt, train_loader, melspec_train, sconfig.device, sconfig.T, sconfig.a)\n",
    "\n",
    "        au_fa_fr = dkd_validation(teacher, student, val_loader,\n",
    "                          melspec_val, sconfig.device, sconfig.T, sconfig.a)\n",
    "        history['val_metric'].append(au_fa_fr)\n",
    "\n",
    "    clear_output()\n",
    "    print(f\"{timer.name.capitalize()} | Elapsed time : {timer.total_time:.2f}\")\n",
    "    plt.plot(history['val_metric'])\n",
    "    plt.ylabel('Metric')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.grid()\n",
    "    plt.savefig('dkd_student_small_attention.png')\n",
    "    plt.show()\n",
    "\n",
    "    print('END OF EPOCH', n)\n",
    "    if au_fa_fr < 5e-5 * 1.1:\n",
    "        print('The student reached the baseline')\n",
    "        torch.save(model, 'student_model.pth')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfl6OkIVD3Q7",
    "outputId": "9e359eb7-3edb-4947-8d8c-7c12ebb13e10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'val_metric': [0.0006763857130346606,\n",
       "              0.00033787060894794967,\n",
       "              0.00020959817369173826,\n",
       "              0.00016745535267385667,\n",
       "              0.00013564832406647788,\n",
       "              0.00013113089054756874,\n",
       "              9.751569502311007e-05,\n",
       "              8.94475706934335e-05,\n",
       "              8.645782935529152e-05,\n",
       "              9.598203529476179e-05,\n",
       "              8.220296792995172e-05,\n",
       "              8.945353824101461e-05,\n",
       "              7.380066093573234e-05,\n",
       "              9.150040706133938e-05,\n",
       "              6.934887044021554e-05,\n",
       "              6.926532477407983e-05,\n",
       "              8.254908568965678e-05,\n",
       "              8.644589426012928e-05,\n",
       "              9.520625410921598e-05,\n",
       "              7.448096135998022e-05,\n",
       "              7.68620128448478e-05,\n",
       "              8.056785989272438e-05,\n",
       "              5.618446047626102e-05,\n",
       "              7.738118948440539e-05,\n",
       "              6.512981430036248e-05,\n",
       "              5.1750572623487585e-05]})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
