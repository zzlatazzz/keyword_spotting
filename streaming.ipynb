{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lhrn5O-qUYZ"
      },
      "source": [
        "# Import and misc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "meO-Mp9jiAFC"
      },
      "outputs": [],
      "source": [
        "# Instal latest torch and torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bbUpoArCqUYa"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, Union, List, Callable, Optional\n",
        "from tqdm import tqdm\n",
        "from itertools import islice\n",
        "import pathlib\n",
        "import dataclasses\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import distributions\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import torchaudio\n",
        "from IPython import display as display_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "812GwLfqqUYf"
      },
      "source": [
        "# Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8PdhApeEh9pH"
      },
      "outputs": [],
      "source": [
        "@dataclasses.dataclass\n",
        "class TaskConfig:\n",
        "    keyword: str = 'sheila'  # We will use 1 key word -- 'sheila'\n",
        "    batch_size: int = 128\n",
        "    learning_rate: float = 3e-4\n",
        "    weight_decay: float = 1e-5\n",
        "    num_epochs: int = 20\n",
        "    n_mels: int = 40\n",
        "    cnn_out_channels: int = 8\n",
        "    kernel_size: Tuple[int, int] = (5, 20)\n",
        "    stride: Tuple[int, int] = (2, 8)\n",
        "    hidden_size: int = 64\n",
        "    gru_num_layers: int = 2\n",
        "    bidirectional: bool = False\n",
        "    num_classes: int = 2\n",
        "    sample_rate: int = 16000\n",
        "    device: torch.device = torch.device(\n",
        "        'cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA1gPmE1h9pI"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2N8zcx9MF1X",
        "outputId": "31449005-ed74-4aec-9d81-86570fe5b794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-05 19:51:50--  http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 108.177.11.128, 2607:f8b0:400c:c01::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|108.177.11.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1489096277 (1.4G) [application/gzip]\n",
            "Saving to: ‘speech_commands_v0.01.tar.gz’\n",
            "\n",
            "speech_commands_v0. 100%[===================>]   1.39G   188MB/s    in 7.5s    \n",
            "\n",
            "2022-11-05 19:51:58 (190 MB/s) - ‘speech_commands_v0.01.tar.gz’ saved [1489096277/1489096277]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz -O speech_commands_v0.01.tar.gz\n",
        "!mkdir speech_commands && tar -C speech_commands -xvzf speech_commands_v0.01.tar.gz 1> log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "12wBTK0mNUsG"
      },
      "outputs": [],
      "source": [
        "class SpeechCommandDataset(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        transform: Optional[Callable] = None,\n",
        "        path2dir: str = None,\n",
        "        keywords: Union[str, List[str]] = None,\n",
        "        csv: Optional[pd.DataFrame] = None\n",
        "    ):        \n",
        "        self.transform = transform\n",
        "\n",
        "        if csv is None:\n",
        "            path2dir = pathlib.Path(path2dir)\n",
        "            keywords = keywords if isinstance(keywords, list) else [keywords]\n",
        "            \n",
        "            all_keywords = [\n",
        "                p.stem for p in path2dir.glob('*')\n",
        "                if p.is_dir() and not p.stem.startswith('_')\n",
        "            ]\n",
        "\n",
        "            triplets = []\n",
        "            for keyword in all_keywords:\n",
        "                paths = (path2dir / keyword).rglob('*.wav')\n",
        "                if keyword in keywords:\n",
        "                    for path2wav in paths:\n",
        "                        triplets.append((path2wav.as_posix(), keyword, 1))\n",
        "                else:\n",
        "                    for path2wav in paths:\n",
        "                        triplets.append((path2wav.as_posix(), keyword, 0))\n",
        "            \n",
        "            self.csv = pd.DataFrame(\n",
        "                triplets,\n",
        "                columns=['path', 'keyword', 'label']\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            self.csv = csv\n",
        "    \n",
        "    def __getitem__(self, index: int):\n",
        "        instance = self.csv.iloc[index]\n",
        "\n",
        "        path2wav = instance['path']\n",
        "        wav, sr = torchaudio.load(path2wav)\n",
        "        wav = wav.sum(dim=0)\n",
        "        \n",
        "        if self.transform:\n",
        "            wav = self.transform(wav)\n",
        "\n",
        "        return {\n",
        "            'wav': wav,\n",
        "            'keywors': instance['keyword'],\n",
        "            'label': instance['label']\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-1rVkT81Pk90"
      },
      "outputs": [],
      "source": [
        "dataset = SpeechCommandDataset(\n",
        "    path2dir='speech_commands', keywords=TaskConfig.keyword\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "DFwhAXdfQLIA",
        "outputId": "6f5002c1-0544-45bd-d403-dfd9f5dca104"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              path keyword  label\n",
              "35390  speech_commands/house/15c563d7_nohash_0.wav   house      0\n",
              "40158   speech_commands/zero/db43cd03_nohash_0.wav    zero      0\n",
              "40641    speech_commands/one/3d53244b_nohash_0.wav     one      0\n",
              "53703    speech_commands/six/0a9f9af7_nohash_0.wav     six      0\n",
              "10300    speech_commands/two/71aa5b54_nohash_0.wav     two      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cac1baf8-a4d3-4e42-858f-8037c532408a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>keyword</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>35390</th>\n",
              "      <td>speech_commands/house/15c563d7_nohash_0.wav</td>\n",
              "      <td>house</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40158</th>\n",
              "      <td>speech_commands/zero/db43cd03_nohash_0.wav</td>\n",
              "      <td>zero</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40641</th>\n",
              "      <td>speech_commands/one/3d53244b_nohash_0.wav</td>\n",
              "      <td>one</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53703</th>\n",
              "      <td>speech_commands/six/0a9f9af7_nohash_0.wav</td>\n",
              "      <td>six</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10300</th>\n",
              "      <td>speech_commands/two/71aa5b54_nohash_0.wav</td>\n",
              "      <td>two</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cac1baf8-a4d3-4e42-858f-8037c532408a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cac1baf8-a4d3-4e42-858f-8037c532408a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cac1baf8-a4d3-4e42-858f-8037c532408a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "dataset.csv.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUxfDJw1qUYi"
      },
      "source": [
        "### Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dkmkxPWQqUYe"
      },
      "outputs": [],
      "source": [
        "class AugsCreation:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.background_noises = [\n",
        "            'speech_commands/_background_noise_/white_noise.wav',\n",
        "            'speech_commands/_background_noise_/dude_miaowing.wav',\n",
        "            'speech_commands/_background_noise_/doing_the_dishes.wav',\n",
        "            'speech_commands/_background_noise_/exercise_bike.wav',\n",
        "            'speech_commands/_background_noise_/pink_noise.wav',\n",
        "            'speech_commands/_background_noise_/running_tap.wav'\n",
        "        ]\n",
        "\n",
        "        self.noises = [\n",
        "            torchaudio.load(p)[0].squeeze()\n",
        "            for p in self.background_noises\n",
        "        ]\n",
        "\n",
        "    def add_rand_noise(self, audio):\n",
        "\n",
        "        # randomly choose noise\n",
        "        noise_num = torch.randint(low=0, high=len(\n",
        "            self.background_noises), size=(1,)).item()\n",
        "        noise = self.noises[noise_num]\n",
        "\n",
        "        noise_level = torch.Tensor([1])  # [0, 40]\n",
        "\n",
        "        noise_energy = torch.norm(noise)\n",
        "        audio_energy = torch.norm(audio)\n",
        "        alpha = (audio_energy / noise_energy) * \\\n",
        "            torch.pow(10, -noise_level / 20)\n",
        "\n",
        "        start = torch.randint(\n",
        "            low=0,\n",
        "            high=max(int(noise.size(0) - audio.size(0) - 1), 1),\n",
        "            size=(1,)\n",
        "        ).item()\n",
        "        noise_sample = noise[start: start + audio.size(0)]\n",
        "\n",
        "        audio_new = audio + alpha * noise_sample\n",
        "        audio_new.clamp_(-1, 1)\n",
        "        return audio_new\n",
        "\n",
        "    def __call__(self, wav):\n",
        "        aug_num = torch.randint(low=0, high=4, size=(1,)).item()   # choose 1 random aug from augs\n",
        "        augs = [\n",
        "            lambda x: x,\n",
        "            lambda x: (x + distributions.Normal(0, 0.01).sample(x.size())).clamp_(-1, 1),\n",
        "            lambda x: torchaudio.transforms.Vol(.25)(x),\n",
        "            lambda x: self.add_rand_noise(x)\n",
        "        ]\n",
        "\n",
        "        return augs[aug_num](wav)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ClWThxyYh9pM"
      },
      "outputs": [],
      "source": [
        "indexes = torch.randperm(len(dataset))\n",
        "train_indexes = indexes[:int(len(dataset) * 0.8)]\n",
        "val_indexes = indexes[int(len(dataset) * 0.8):]\n",
        "\n",
        "train_df = dataset.csv.iloc[train_indexes].reset_index(drop=True)\n",
        "val_df = dataset.csv.iloc[val_indexes].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PDPLht5fqUYe"
      },
      "outputs": [],
      "source": [
        "# Sample is a dict of utt, word and label\n",
        "train_set = SpeechCommandDataset(csv=train_df, transform=AugsCreation())\n",
        "val_set = SpeechCommandDataset(csv=val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmrJd8WIhkLP",
        "outputId": "b0e7ec7a-c236-4a59-a90c-d7e3e6efd8c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'wav': tensor([0.0010, 0.0020, 0.0022,  ..., 0.0007, 0.0004, 0.0006]),\n",
              " 'keywors': 'up',\n",
              " 'label': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train_set[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vbPDqd6qUYj"
      },
      "source": [
        "### Sampler for oversampling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rfnjRKo2qUYj"
      },
      "outputs": [],
      "source": [
        "# We should provide to WeightedRandomSampler _weight for every sample_; by default it is 1/len(target)\n",
        "\n",
        "def get_sampler(target):\n",
        "    class_sample_count = np.array(\n",
        "        [len(np.where(target == t)[0]) for t in np.unique(target)])   # for every class count it's number of occ.\n",
        "    weight = 1. / class_sample_count\n",
        "    samples_weight = np.array([weight[t] for t in target])\n",
        "    samples_weight = torch.from_numpy(samples_weight)\n",
        "    samples_weigth = samples_weight.float()\n",
        "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
        "    return sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UM8gLmHeqUYj"
      },
      "outputs": [],
      "source": [
        "train_sampler = get_sampler(train_set.csv['label'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lyBqbxp0h9pO"
      },
      "outputs": [],
      "source": [
        "class Collator:\n",
        "    \n",
        "    def __call__(self, data):\n",
        "        wavs = []\n",
        "        labels = []    \n",
        "\n",
        "        for el in data:\n",
        "            wavs.append(el['wav'])\n",
        "            labels.append(el['label'])\n",
        "\n",
        "        # torch.nn.utils.rnn.pad_sequence takes list(Tensors) and returns padded (with 0.0) Tensor\n",
        "        wavs = pad_sequence(wavs, batch_first=True)    \n",
        "        labels = torch.Tensor(labels).long()\n",
        "        return wavs, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8G9xPRVqUYk"
      },
      "source": [
        "###  Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6wGBMcQiqUYk"
      },
      "outputs": [],
      "source": [
        "# Here we are obliged to use shuffle=False because of our sampler with randomness inside.\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=TaskConfig.batch_size,\n",
        "                          shuffle=False, collate_fn=Collator(),\n",
        "                          sampler=train_sampler,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "val_loader = DataLoader(val_set, batch_size=TaskConfig.batch_size,\n",
        "                        shuffle=False, collate_fn=Collator(),\n",
        "                        num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTlsn6cpqUYk"
      },
      "source": [
        "### Creating MelSpecs on GPU for speeeed: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pRXMt6it56fW"
      },
      "outputs": [],
      "source": [
        "class LogMelspec:\n",
        "\n",
        "    def __init__(self, is_train, config):\n",
        "        # with augmentations\n",
        "        if is_train:\n",
        "            self.melspec = nn.Sequential(\n",
        "                torchaudio.transforms.MelSpectrogram(\n",
        "                    sample_rate=config.sample_rate,\n",
        "                    n_fft=400,\n",
        "                    win_length=400,\n",
        "                    hop_length=160,\n",
        "                    n_mels=config.n_mels\n",
        "                ),\n",
        "                torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
        "                torchaudio.transforms.TimeMasking(time_mask_param=35),\n",
        "            ).to(config.device)\n",
        "\n",
        "        # no augmentations\n",
        "        else:\n",
        "            self.melspec = torchaudio.transforms.MelSpectrogram(\n",
        "                sample_rate=config.sample_rate,\n",
        "                n_fft=400,\n",
        "                win_length=400,\n",
        "                hop_length=160,\n",
        "                n_mels=config.n_mels\n",
        "            ).to(config.device)\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # already on device\n",
        "        return torch.log(self.melspec(batch).clamp_(min=1e-9, max=1e9))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Pqkz4_gn8BiF"
      },
      "outputs": [],
      "source": [
        "melspec_train = LogMelspec(is_train=True, config=TaskConfig)\n",
        "melspec_val = LogMelspec(is_train=False, config=TaskConfig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPMgfwowi3X-"
      },
      "source": [
        "# Streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "g9QI7Kn1wKv6"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.energy = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, input):\n",
        "        energy = self.energy(input)\n",
        "        alpha = torch.softmax(energy, dim=-2)\n",
        "        return (input * alpha).sum(dim=-2)\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, config: TaskConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1, out_channels=config.cnn_out_channels,\n",
        "                kernel_size=config.kernel_size, stride=config.stride\n",
        "            ),\n",
        "            nn.Flatten(start_dim=1, end_dim=2),\n",
        "        )\n",
        "\n",
        "        self.conv_out_frequency = (config.n_mels - config.kernel_size[0]) // \\\n",
        "            config.stride[0] + 1\n",
        "        \n",
        "        self.gru = nn.GRU(\n",
        "            input_size=self.conv_out_frequency * config.cnn_out_channels,\n",
        "            hidden_size=config.hidden_size,\n",
        "            num_layers=config.gru_num_layers,\n",
        "            dropout=0.1,\n",
        "            bidirectional=config.bidirectional,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.attention = Attention(config.hidden_size)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        input = input.unsqueeze(dim=1)\n",
        "        conv_output = self.conv(input).transpose(-1, -2)\n",
        "        gru_output, _ = self.gru(conv_output)\n",
        "        contex_vector = self.attention(gru_output)\n",
        "        output = self.classifier(contex_vector)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PpyvKwp0k3IU"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JUF974hhwBDG"
      },
      "outputs": [],
      "source": [
        "class StreamCRNN(CRNN):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "    \n",
        "    def stream_forward(self, input, hidden=None):\n",
        "        input = input.unsqueeze(dim=1)\n",
        "        conv_output = self.conv(input).transpose(-1, -2)\n",
        "        gru_output, hidden = self.gru(conv_output, hidden) if hidden is not None else self.gru(conv_output)\n",
        "        contex_vector = self.attention(gru_output)\n",
        "        output = self.classifier(contex_vector)\n",
        "        return output, hidden\n",
        "\n",
        "    \n",
        "class Streaming():\n",
        "    def __init__(self,  max_window_length=40, streaming_step_size=2):\n",
        "        self.max_window_length = max_window_length\n",
        "        self.streaming_step_size = streaming_step_size\n",
        "        \n",
        "    def stream(self, model, input):\n",
        "        model.eval()\n",
        "        kernel_width = model.conv[0].kernel_size[1]\n",
        "        stride = model.conv[0].stride[-1]\n",
        "        max_window_length = self.max_window_length - (self.max_window_length - kernel_width) % stride\n",
        "        hidden = None\n",
        "        \n",
        "        output, hidden = model.stream_forward(input[:,:,:max_window_length])\n",
        "        result = output.unsqueeze(1)\n",
        "\n",
        "        for i in range(max_window_length, input.shape[-1], self.streaming_step_size):\n",
        "            output, hidden = model.stream_forward(input[:,:, i - max_window_length + 1 : i + 1], hidden=hidden)\n",
        "            output = output.unsqueeze(1)\n",
        "            result = torch.cat((result, output[:, -1:]), dim=1)\n",
        "        probs = F.softmax(result, dim=-1)[0].detach().numpy()\n",
        "        clear_output()\n",
        "        plt.plot(list(range(self.max_window_length // 2, probs.shape[0] * self.streaming_step_size + self.max_window_length // 2, self.streaming_step_size)), probs[:,1])\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclasses.dataclass\n",
        "class StudentConfig:\n",
        "    keyword: str = 'sheila'  # We will use 1 key word -- 'sheila'\n",
        "    batch_size: int = 128\n",
        "    learning_rate: float = 1e-3 # 3e-4\n",
        "    weight_decay: float = 1e-5 # 1e-5\n",
        "    num_epochs: int = 40 # 20\n",
        "    n_mels: int = 40\n",
        "    cnn_out_channels: int = 2 # 8\n",
        "    kernel_size: Tuple[int, int] = (5, 20) # (5, 20)\n",
        "    stride: Tuple[int, int] = (2, 8) # (2, 8)\n",
        "    hidden_size: int = 22 # 64\n",
        "    gru_num_layers: int = 1 # 2\n",
        "    bidirectional: bool = False\n",
        "    num_classes: int = 2\n",
        "    sample_rate: int = 16000\n",
        "    device: torch.device = torch.device(\n",
        "        'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    T: float = 10\n",
        "    a: float = 0.6"
      ],
      "metadata": {
        "id": "J6JLak3MWXWG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJBVhsu1tIAc",
        "outputId": "7bc42d90-400c-4bbd-a314-f8b84228fa2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StreamCRNN(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(1, 2, kernel_size=(5, 20), stride=(2, 8))\n",
              "    (1): Flatten(start_dim=1, end_dim=2)\n",
              "  )\n",
              "  (gru): GRU(36, 22, batch_first=True, dropout=0.1)\n",
              "  (attention): Attention(\n",
              "    (energy): Sequential(\n",
              "      (0): Linear(in_features=22, out_features=22, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=22, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=22, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "sconfig = StudentConfig()\n",
        "model = torch.load(\"student_model.pth\", map_location=torch.device('cpu')).eval()\n",
        "torch.save(model.state_dict(), 'student_model.pt')\n",
        "\n",
        "model = StreamCRNN(sconfig)\n",
        "model.load_state_dict(torch.load('student_model.pt', map_location='cpu'))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rLqKKfJytyGa",
        "outputId": "92da7e91-3cc7-47ec-c45a-1db9691a0ed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "69\n",
            "99\n",
            "148\n",
            "207\n",
            "216\n",
            "241\n",
            "284\n",
            "317\n",
            "338\n",
            "363\n",
            "372\n",
            "383\n",
            "402\n",
            "424\n",
            "484\n",
            "529\n",
            "542\n",
            "609\n",
            "616\n",
            "672\n",
            "680\n",
            "704\n",
            "765\n",
            "789\n",
            "888\n",
            "938\n",
            "1026\n",
            "1038\n",
            "1070\n",
            "1072\n",
            "1089\n",
            "1104\n",
            "1160\n",
            "1224\n",
            "1281\n",
            "1305\n",
            "1323\n",
            "1325\n",
            "1339\n",
            "1444\n",
            "1522\n",
            "1574\n",
            "1594\n",
            "1606\n",
            "1682\n",
            "1764\n",
            "1765\n",
            "1772\n",
            "1850\n",
            "1856\n",
            "1932\n",
            "1944\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-528cefde5296>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-c9aefe7be7b4>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mpath2wav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath2wav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchaudio/backend/sox_io_backend.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     ret = torch.ops.torchaudio.sox_io_load_audio_file(\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     )\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# We save the function ptr as the `op` attribute on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# OpOverloadPacket to access it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for i, t in enumerate(train_set):\n",
        "    if t['label'] == 1:\n",
        "        print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "F9yRdNfyK9vE"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def seed_all(seed_value):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    \n",
        "    if torch.cuda.is_available(): \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "9o706a-huL-O",
        "outputId": "7d5c9508-00c6-4fb1-a907-c8d30254545f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "seed_all(112)\n",
        "\n",
        "st = Streaming(max_window_length = 100, streaming_step_size=5)\n",
        "\n",
        "wav, sr = torchaudio.load('audio.wav')\n",
        "wav = wav.sum(dim=0)\n",
        "mel = melspec_val(wav.unsqueeze(0).to(sconfig.device))\n",
        "result = st.stream(model, mel)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}